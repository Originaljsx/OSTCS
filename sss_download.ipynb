{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3167f96a-8bc3-4577-a18b-8040806261ca",
   "metadata": {
    "id": "3167f96a-8bc3-4577-a18b-8040806261ca"
   },
   "outputs": [],
   "source": [
    "# Create a _netrc file\n",
    "#!echo \"machine urs.earthdata.nasa.gov login YOURUSERNAME password YOURPASSWORD\" > ~/_netrc\n",
    "#!chmod 600 ~/_netrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f600aaa-7253-487b-8446-e2ec32d960a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5f600aaa-7253-487b-8446-e2ec32d960a0",
    "outputId": "ce81ddc3-7d2a-4a7d-fe07-c13842c86154"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download directory: Earth_Data\\OISSS_L4_multimission_monthly_v2\n",
      "Searching for OISSS_L4_multimission_monthly_v2 data between 2016-01-01 and 2016-12-31\n",
      "Found 9 matching granules\n",
      "First file: https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/OISSS_L4_multimission_monthly_v2/OISSS_L4_multimission_global_monthly_v2.0_2016-01.nc\n",
      "Starting download...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:06<00:00,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====================================\n",
      "total downloaded: 61.97 Mb\n",
      "avg download speed: 9.57 Mb/s\n",
      "Total downloaded files: 20\n",
      "You can now open the files for analysis with:\n",
      "xds = xr.open_mfdataset(salinity_files, parallel=True)\n",
      "xds.SSS.mean('time').plot(figsize=[20,10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import shutil\n",
    "import time\n",
    "import xarray as xr\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from getpass import getpass\n",
    "from http.cookiejar import CookieJar\n",
    "from io import StringIO\n",
    "from itertools import repeat\n",
    "from pathlib import Path\n",
    "from platform import system\n",
    "from netrc import netrc\n",
    "from os.path import expanduser, basename, isfile, isdir, join\n",
    "from tqdm import tqdm\n",
    "from urllib import request\n",
    "\n",
    "_netrc = join(expanduser('~'), \"_netrc\" if system()==\"Windows\" else \".netrc\")\n",
    "\n",
    "# ----- Helper Subroutines -----\n",
    "\n",
    "# Function to log into NASA EarthData\n",
    "def setup_earthdata_login_auth(url: str='urs.earthdata.nasa.gov'):\n",
    "    # look for the netrc file and use the login/password\n",
    "    try:\n",
    "        username, _, password = netrc(file=_netrc).authenticators(url)\n",
    "\n",
    "    # if file is not found, prompt user for the login/password\n",
    "    except (FileNotFoundError, TypeError):\n",
    "        print('Please provide Earthdata Login credentials for access.')\n",
    "        username, password = input('Username: '), getpass('Password: ')\n",
    "\n",
    "    manager = request.HTTPPasswordMgrWithDefaultRealm()\n",
    "    manager.add_password(None, url, username, password)\n",
    "    auth = request.HTTPBasicAuthHandler(manager)\n",
    "    jar = CookieJar()\n",
    "    processor = request.HTTPCookieProcessor(jar)\n",
    "    opener = request.build_opener(auth, processor)\n",
    "    request.install_opener(opener)\n",
    "\n",
    "# Functions to make API calls\n",
    "def set_params(params: dict):\n",
    "    params.update({'scroll': \"true\", 'page_size': 2000})\n",
    "    return {par: val for par, val in params.items() if val is not None}\n",
    "\n",
    "def get_results(params: dict, headers: dict=None):\n",
    "    response = requests.get(url=\"https://cmr.earthdata.nasa.gov/search/granules.csv\",\n",
    "                            params=set_params(params),\n",
    "                            headers=headers)\n",
    "    return response, response.headers\n",
    "\n",
    "def get_granules(params: dict):\n",
    "    response, headers = get_results(params=params)\n",
    "    scroll = headers['CMR-Scroll-Id']\n",
    "    hits = int(headers['CMR-Hits'])\n",
    "    if hits==0:\n",
    "        raise Exception(\"No granules matched your input parameters.\")\n",
    "    df = pd.read_csv(StringIO(response.text))\n",
    "    while hits > df.index.size:\n",
    "        response, _ = get_results(params=params, headers={'CMR-Scroll-Id': scroll})\n",
    "        data = pd.read_csv(StringIO(response.text))\n",
    "        df = pd.concat([df, data])\n",
    "    return df\n",
    "\n",
    "# Function to download single files\n",
    "def download_file(url: str, output_dir: str, force: bool=False):\n",
    "    \"\"\"\n",
    "    url (str): the HTTPS url from which the file will download\n",
    "    output_dir (str): the local path into which the file will download\n",
    "    force (bool): download even if the file exists locally already\n",
    "    \"\"\"\n",
    "    if not isdir(output_dir):\n",
    "        raise Exception(f\"Output directory doesn't exist! ({output_dir})\")\n",
    "\n",
    "    target_file = join(output_dir, basename(url))\n",
    "\n",
    "    # if the file has already been downloaded, skip\n",
    "    if isfile(target_file) and force is False:\n",
    "        print(f'\\n{basename(url)} already exists, and force=False, not re-downloading')\n",
    "        return 0\n",
    "\n",
    "    with requests.get(url) as r:\n",
    "        if not r.status_code // 100 == 2:\n",
    "            raise Exception(r.text)\n",
    "            return 0\n",
    "        else:\n",
    "            with open(target_file, 'wb') as f:\n",
    "                total_size_in_bytes= int(r.headers.get('content-length', 0))\n",
    "                for chunk in r.iter_content(chunk_size=1024):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "\n",
    "                return total_size_in_bytes\n",
    "\n",
    "# Function to download all URLs concurrently\n",
    "def download_files_concurrently(dls, download_dir, force=False, max_workers=6):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Use multiple threads for concurrent downloads\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # progress bar\n",
    "        results = list(tqdm(executor.map(download_file, dls, repeat(download_dir), repeat(force)), total=len(dls)))\n",
    "\n",
    "        # add up the total downloaded file sizes\n",
    "        total_download_size_in_bytes = np.sum(np.array(results))\n",
    "        # calculate total time spent in the download\n",
    "        total_time = time.time() - start_time\n",
    "\n",
    "        print('\\n=====================================')\n",
    "        print(f'total downloaded: {np.round(total_download_size_in_bytes/1e6,2)} Mb')\n",
    "        print(f'avg download speed: {np.round(total_download_size_in_bytes/1e6/total_time,2)} Mb/s')\n",
    "\n",
    "# ----- Main Script -----\n",
    "\n",
    "# OISSS dataset shortname\n",
    "ShortName = \"OISSS_L4_multimission_monthly_v2\"\n",
    "\n",
    "# Specify desired date range - adjust as needed\n",
    "# This dataset starts from August 2011\n",
    "StartDate = \"2016-01-01\"\n",
    "EndDate = \"2016-12-31\"\n",
    "\n",
    "# Define download directory\n",
    "download_root_dir = Path('./Earth_Data')  # Change this for directory\n",
    "download_dir = download_root_dir / ShortName\n",
    "\n",
    "# Create the download directory\n",
    "download_dir.mkdir(exist_ok=True, parents=True)\n",
    "print(f'Download directory: {download_dir}')\n",
    "\n",
    "# Log into Earthdata\n",
    "setup_earthdata_login_auth()\n",
    "\n",
    "# Search parameters for the dataset\n",
    "input_search_params = {\n",
    "    'ShortName': ShortName,\n",
    "    'temporal': \",\".join([StartDate, EndDate])\n",
    "}\n",
    "\n",
    "print(f\"Searching for {ShortName} data between {StartDate} and {EndDate}\")\n",
    "\n",
    "# Query CMR for the OISSS Dataset\n",
    "try:\n",
    "    grans = get_granules(input_search_params)\n",
    "    num_grans = len(grans['Granule UR'])\n",
    "    print(f'Found {num_grans} matching granules')\n",
    "\n",
    "    # Convert the URLs to a list\n",
    "    dls = grans['Online Access URLs'].tolist()\n",
    "\n",
    "    if num_grans > 0:\n",
    "        print(f'First file: {dls[0]}')\n",
    "\n",
    "        # Download the granules with concurrent downloads\n",
    "        max_workers = 6  # Adjust based on your internet connection\n",
    "        force = True    # Set to True to force redownload of existing files\n",
    "\n",
    "        print(\"Starting download...\")\n",
    "        download_files_concurrently(dls, download_dir, force, max_workers)\n",
    "\n",
    "        # List downloaded files\n",
    "        salinity_files = list(download_dir.glob('*nc'))\n",
    "        print(f'Total downloaded files: {len(salinity_files)}')\n",
    "\n",
    "        # Optionally, open and plot data\n",
    "        if len(salinity_files) > 0:\n",
    "            print(\"You can now open the files for analysis with:\")\n",
    "            print(\"xds = xr.open_mfdataset(salinity_files, parallel=True)\")\n",
    "            print(\"xds.SSS.mean('time').plot(figsize=[20,10])\")\n",
    "    else:\n",
    "        print(\"No files found for the specified date range.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18209e80-e4e9-4fce-a8ce-684e850625df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "18209e80-e4e9-4fce-a8ce-684e850625df",
    "outputId": "d4796f1b-fd98-4703-964c-b5c36cd5dfe5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jspier\\pre\\Earth_Data\\OISSS_L4_multimission_monthly_v2\n",
      "Earth_Data\\OISSS_L4_multimission_monthly_v2\\OISSS_L4_multimission_global_monthly_v2.0_2015-01.nc\n",
      "Earth_Data\\OISSS_L4_multimission_monthly_v2\\OISSS_L4_multimission_global_monthly_v2.0_2015-02.nc\n",
      "Earth_Data\\OISSS_L4_multimission_monthly_v2\\OISSS_L4_multimission_global_monthly_v2.0_2015-03.nc\n",
      "Earth_Data\\OISSS_L4_multimission_monthly_v2\\OISSS_L4_multimission_global_monthly_v2.0_2015-04.nc\n",
      "Earth_Data\\OISSS_L4_multimission_monthly_v2\\OISSS_L4_multimission_global_monthly_v2.0_2015-05.nc\n",
      "Earth_Data\\OISSS_L4_multimission_monthly_v2\\OISSS_L4_multimission_global_monthly_v2.0_2015-06.nc\n",
      "Earth_Data\\OISSS_L4_multimission_monthly_v2\\OISSS_L4_multimission_global_monthly_v2.0_2015-08.nc\n",
      "Earth_Data\\OISSS_L4_multimission_monthly_v2\\OISSS_L4_multimission_global_monthly_v2.0_2015-09.nc\n",
      "Earth_Data\\OISSS_L4_multimission_monthly_v2\\OISSS_L4_multimission_global_monthly_v2.0_2015-10.nc\n",
      "Earth_Data\\OISSS_L4_multimission_monthly_v2\\OISSS_L4_multimission_global_monthly_v2.0_2015-11.nc\n",
      "Earth_Data\\OISSS_L4_multimission_monthly_v2\\OISSS_L4_multimission_global_monthly_v2.0_2015-12.nc\n",
      "Earth_Data\\OISSS_L4_multimission_monthly_v2\\OISSS_L4_multimission_global_monthly_v2.0_2016-01.nc\n",
      "Earth_Data\\OISSS_L4_multimission_monthly_v2\\OISSS_L4_multimission_global_monthly_v2.0_2016-03.nc\n",
      "Earth_Data\\OISSS_L4_multimission_monthly_v2\\OISSS_L4_multimission_global_monthly_v2.0_2016-05.nc\n",
      "Earth_Data\\OISSS_L4_multimission_monthly_v2\\OISSS_L4_multimission_global_monthly_v2.0_2016-06.nc\n",
      "Earth_Data\\OISSS_L4_multimission_monthly_v2\\OISSS_L4_multimission_global_monthly_v2.0_2016-07.nc\n",
      "Earth_Data\\OISSS_L4_multimission_monthly_v2\\OISSS_L4_multimission_global_monthly_v2.0_2016-08.nc\n",
      "Earth_Data\\OISSS_L4_multimission_monthly_v2\\OISSS_L4_multimission_global_monthly_v2.0_2016-09.nc\n",
      "Earth_Data\\OISSS_L4_multimission_monthly_v2\\OISSS_L4_multimission_global_monthly_v2.0_2016-10.nc\n",
      "Earth_Data\\OISSS_L4_multimission_monthly_v2\\OISSS_L4_multimission_global_monthly_v2.0_2016-12.nc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# This will print the absolute path\n",
    "print(os.path.abspath(\"Earth_Data/OISSS_L4_multimission_monthly_v2\"))\n",
    "\n",
    "# List the files to confirm they're there\n",
    "files = list(Path(\"Earth_Data/OISSS_L4_multimission_monthly_v2\").glob(\"*.nc\"))\n",
    "for file in files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca0d6950-fcf6-40ba-85b4-eeb9e219d622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download directory: C:\\Users\\jspier\\pre\\Earth_Data\\OISSS_L4_multimission_monthly_v2\n",
      "Using credentials from netrc file: C:\\Users\\jspier\\_netrc\n",
      "\n",
      "Searching CMR for OISSS_L4_multimission_monthly_v2 data between 2016-01-01 and 2016-12-31...\n",
      "Initial CMR search parameters being used: {'ShortName': 'OISSS_L4_multimission_monthly_v2', 'temporal': '2016-01-01,2016-12-31'}\n",
      "CMR reported 9 total matching granules.\n",
      "Found 9 granules on the first page.\n",
      "\n",
      "CMR search yielded 9 granules with downloadable URLs.\n",
      "\n",
      "URLs to be downloaded:\n",
      "1: https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/OISSS_L4_multimission_monthly_v2/OISSS_L4_multimission_global_monthly_v2.0_2016-01.nc\n",
      "2: https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/OISSS_L4_multimission_monthly_v2/OISSS_L4_multimission_global_monthly_v2.0_2016-03.nc\n",
      "3: https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/OISSS_L4_multimission_monthly_v2/OISSS_L4_multimission_global_monthly_v2.0_2016-05.nc\n",
      "4: https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/OISSS_L4_multimission_monthly_v2/OISSS_L4_multimission_global_monthly_v2.0_2016-06.nc\n",
      "5: https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/OISSS_L4_multimission_monthly_v2/OISSS_L4_multimission_global_monthly_v2.0_2016-07.nc\n",
      "6: https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/OISSS_L4_multimission_monthly_v2/OISSS_L4_multimission_global_monthly_v2.0_2016-08.nc\n",
      "7: https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/OISSS_L4_multimission_monthly_v2/OISSS_L4_multimission_global_monthly_v2.0_2016-09.nc\n",
      "8: https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/OISSS_L4_multimission_monthly_v2/OISSS_L4_multimission_global_monthly_v2.0_2016-10.nc\n",
      "9: https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/OISSS_L4_multimission_monthly_v2/OISSS_L4_multimission_global_monthly_v2.0_2016-12.nc\n",
      "Expected number of monthly granules for the period: 12\n",
      "Warning: Found fewer granules (9) than expected (12). Some data may be missing from the source for this period.\n",
      "\n",
      "Proceeding to download 9 files. Force redownload: False\n",
      "\n",
      "Starting concurrent download of 9 files to Earth_Data\\OISSS_L4_multimission_monthly_v2 with 4 workers...\n",
      "\n",
      "OISSS_L4_multimission_global_monthly_v2.0_2016-01.nc already exists, and force=False, not re-downloading.\n",
      "\n",
      "OISSS_L4_multimission_global_monthly_v2.0_2016-03.nc already exists, and force=False, not re-downloading.\n",
      "\n",
      "OISSS_L4_multimission_global_monthly_v2.0_2016-06.nc already exists, and force=False, not re-downloading.\n",
      "\n",
      "OISSS_L4_multimission_global_monthly_v2.0_2016-05.nc already exists, and force=False, not re-downloading.\n",
      "\n",
      "OISSS_L4_multimission_global_monthly_v2.0_2016-07.nc already exists, and force=False, not re-downloading.\n",
      "\n",
      "OISSS_L4_multimission_global_monthly_v2.0_2016-08.nc already exists, and force=False, not re-downloading.\n",
      "\n",
      "OISSS_L4_multimission_global_monthly_v2.0_2016-09.nc already exists, and force=False, not re-downloading.\n",
      "\n",
      "OISSS_L4_multimission_global_monthly_v2.0_2016-10.nc already exists, and force=False, not re-downloading.\n",
      "\n",
      "OISSS_L4_multimission_global_monthly_v2.0_2016-12.nc already exists, and force=False, not re-downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading files: 100%|█████████████████████████████████████████████████████████████████████████| 9/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====================================\n",
      "Download process complete.\n",
      "Successfully downloaded 0/9 files.\n",
      "Total downloaded data: 0.0 MB\n",
      "Average download speed: 0.0 MB/s (for successful downloads)\n",
      "=====================================\n",
      "\n",
      "Successfully downloaded 0 files:\n",
      "\n",
      "Files found in download directory (Earth_Data\\OISSS_L4_multimission_monthly_v2):\n",
      "OISSS_L4_multimission_global_monthly_v2.0_2015-01.nc\n",
      "OISSS_L4_multimission_global_monthly_v2.0_2015-02.nc\n",
      "OISSS_L4_multimission_global_monthly_v2.0_2015-03.nc\n",
      "OISSS_L4_multimission_global_monthly_v2.0_2015-04.nc\n",
      "OISSS_L4_multimission_global_monthly_v2.0_2015-05.nc\n",
      "OISSS_L4_multimission_global_monthly_v2.0_2015-06.nc\n",
      "OISSS_L4_multimission_global_monthly_v2.0_2015-08.nc\n",
      "OISSS_L4_multimission_global_monthly_v2.0_2015-09.nc\n",
      "OISSS_L4_multimission_global_monthly_v2.0_2015-10.nc\n",
      "OISSS_L4_multimission_global_monthly_v2.0_2015-11.nc\n",
      "OISSS_L4_multimission_global_monthly_v2.0_2015-12.nc\n",
      "OISSS_L4_multimission_global_monthly_v2.0_2016-01.nc\n",
      "OISSS_L4_multimission_global_monthly_v2.0_2016-03.nc\n",
      "OISSS_L4_multimission_global_monthly_v2.0_2016-05.nc\n",
      "OISSS_L4_multimission_global_monthly_v2.0_2016-06.nc\n",
      "OISSS_L4_multimission_global_monthly_v2.0_2016-07.nc\n",
      "OISSS_L4_multimission_global_monthly_v2.0_2016-08.nc\n",
      "OISSS_L4_multimission_global_monthly_v2.0_2016-09.nc\n",
      "OISSS_L4_multimission_global_monthly_v2.0_2016-10.nc\n",
      "OISSS_L4_multimission_global_monthly_v2.0_2016-12.nc\n",
      "Total .nc files in directory: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import shutil # Not used in the provided snippet, but kept if originally intended\n",
    "import time\n",
    "import xarray as xr # Not used in the download script part, but kept from original imports\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from getpass import getpass\n",
    "from http.cookiejar import CookieJar\n",
    "from io import StringIO\n",
    "from itertools import repeat\n",
    "from pathlib import Path\n",
    "from platform import system\n",
    "from netrc import netrc\n",
    "from os.path import expanduser, basename, isfile, isdir, join # basename, isfile, isdir, join are used\n",
    "from tqdm import tqdm\n",
    "from urllib import request # Used for auth setup\n",
    "\n",
    "# Determine the netrc file path based on OS\n",
    "_netrc_file = join(expanduser('~'), \"_netrc\" if system()==\"Windows\" else \".netrc\")\n",
    "\n",
    "# ----- Helper Subroutines -----\n",
    "\n",
    "# Function to log into NASA EarthData\n",
    "def setup_earthdata_login_auth(url: str='urs.earthdata.nasa.gov', netrc_path: str=_netrc_file):\n",
    "    \"\"\"Sets up authentication for NASA Earthdata using netrc or user input.\"\"\"\n",
    "    try:\n",
    "        username, _, password = netrc(file=netrc_path).authenticators(url)\n",
    "        if username is None or password is None: # Handles cases where netrc exists but no entry for the url\n",
    "            raise FileNotFoundError # Treat as if netrc didn't have the info\n",
    "        print(f\"Using credentials from netrc file: {netrc_path}\")\n",
    "    except (FileNotFoundError, TypeError, netrc.NetrcParseError):\n",
    "        print(f\"Could not find valid credentials in netrc file ({netrc_path}) or file not found.\")\n",
    "        print('Please provide Earthdata Login credentials for access.')\n",
    "        username, password = input('Username: '), getpass('Password: ')\n",
    "\n",
    "    manager = request.HTTPPasswordMgrWithDefaultRealm()\n",
    "    manager.add_password(None, url, username, password) # The first None means it's a global password for the URL\n",
    "    auth = request.HTTPBasicAuthHandler(manager)\n",
    "    jar = CookieJar() # To handle cookies\n",
    "    processor = request.HTTPCookieProcessor(jar)\n",
    "    opener = request.build_opener(auth, processor)\n",
    "    request.install_opener(opener) # Install this opener for all 'request' calls\n",
    "\n",
    "# Functions to make API calls to CMR\n",
    "def set_params(params: dict):\n",
    "    \"\"\"Updates search parameters with defaults for CMR query.\"\"\"\n",
    "    # Defaults for CMR search, including enabling scrolling and setting a large page size\n",
    "    default_params = {'scroll': \"true\", 'page_size': 2000}\n",
    "    # Update the provided params with these defaults; provided params will override if keys conflict\n",
    "    # before this line, but here we ensure our defaults are set.\n",
    "    # A better way is to start with defaults and update with params.\n",
    "    final_params = default_params.copy()\n",
    "    final_params.update(params)\n",
    "    # Filter out any parameters that are None\n",
    "    return {par: val for par, val in final_params.items() if val is not None}\n",
    "\n",
    "def get_results(params: dict, headers: dict=None):\n",
    "    \"\"\"Submits a GET request to CMR and returns the response and headers.\"\"\"\n",
    "    cmr_url = \"https://cmr.earthdata.nasa.gov/search/granules.csv\"\n",
    "    try:\n",
    "        # Pass processed parameters to the GET request\n",
    "        response = requests.get(url=cmr_url, params=set_params(params), headers=headers)\n",
    "        response.raise_for_status() # Raise an exception for HTTP errors (4xx or 5xx)\n",
    "        return response, response.headers\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP Error during CMR Query: {e}\")\n",
    "        print(f\"Response content: {e.response.text}\")\n",
    "        raise\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request Exception during CMR Query: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def get_granules(params: dict):\n",
    "    \"\"\"\n",
    "    Retrieves granule information from CMR based on search parameters.\n",
    "    Handles scrolling if results span multiple pages.\n",
    "    \"\"\"\n",
    "    print(f\"Initial CMR search parameters being used: {params}\")\n",
    "    response, headers = get_results(params=params) # Initial request\n",
    "\n",
    "    # Extract scroll ID and total hits from response headers\n",
    "    scroll_id = headers.get('CMR-Scroll-Id')\n",
    "    hits = int(headers.get('CMR-Hits', 0))\n",
    "\n",
    "    print(f\"CMR reported {hits} total matching granules.\")\n",
    "\n",
    "    if hits == 0:\n",
    "        raise Exception(\"No granules matched your input parameters according to CMR.\")\n",
    "\n",
    "    # Read the CSV data from the first response\n",
    "    try:\n",
    "        df = pd.read_csv(StringIO(response.text))\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\"CMR returned an empty CSV response for the first page, but reported hits > 0. This is unusual.\")\n",
    "        df = pd.DataFrame() # Start with an empty dataframe\n",
    "\n",
    "    print(f\"Found {len(df)} granules on the first page.\")\n",
    "\n",
    "    # Scroll for more results if needed\n",
    "    # Note: For page_size=2000 and only 12 expected granules, this loop likely won't run.\n",
    "    while hits > len(df.index) and scroll_id:\n",
    "        print(f\"Continuing scroll. Current granules: {len(df)}, Total hits: {hits}, Scroll ID: {scroll_id}\")\n",
    "        # For subsequent scroll requests, CMR expects only the scroll_id in headers.\n",
    "        # Params for get_results should be empty or minimal as the search context is in the scroll_id.\n",
    "        # The `set_params({})` will still add `scroll=true` and `page_size`,\n",
    "        # but CMR should prioritize the scroll_id.\n",
    "        response, headers_scroll = get_results(params={}, headers={'CMR-Scroll-Id': scroll_id})\n",
    "        \n",
    "        # Update scroll_id from the new headers for the next potential iteration\n",
    "        # Some CMR versions might not return a new scroll_id on every scroll page,\n",
    "        # or might expect the same scroll_id to be used. This assumes a new one or reuse of old.\n",
    "        # If headers_scroll.get('CMR-Scroll-Id') is None, it might mean the end of scroll session.\n",
    "        current_scroll_id = headers_scroll.get('CMR-Scroll-Id')\n",
    "        if not current_scroll_id or current_scroll_id != scroll_id: # If scroll_id changes or disappears\n",
    "            scroll_id = current_scroll_id # Update it\n",
    "            if not scroll_id:\n",
    "                print(\"Scroll ID disappeared, stopping scroll.\")\n",
    "                break\n",
    "\n",
    "        try:\n",
    "            data = pd.read_csv(StringIO(response.text))\n",
    "            if not data.empty:\n",
    "                df = pd.concat([df, data], ignore_index=True)\n",
    "                print(f\"Fetched page via scroll, total granules in DataFrame now: {len(df)}\")\n",
    "            else:\n",
    "                print(\"Scroll request returned no new data, stopping scroll.\")\n",
    "                break # No new data\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(\"Scroll request returned an empty CSV, stopping scroll.\")\n",
    "            break # Empty response\n",
    "\n",
    "    if len(df.index) != hits:\n",
    "        print(f\"Warning: Final granule count in DataFrame ({len(df.index)}) does not match CMR-Hits ({hits}).\")\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "# Function to download single files\n",
    "def download_file(url: str, output_dir_path: Path, force: bool=False):\n",
    "    \"\"\"\n",
    "    Downloads a single file from a URL to an output directory.\n",
    "\n",
    "    url (str): The HTTPS URL from which the file will download.\n",
    "    output_dir_path (Path): The local Path object for the directory.\n",
    "    force (bool): Download even if the file exists locally already.\n",
    "    \"\"\"\n",
    "    if not output_dir_path.is_dir():\n",
    "        # This should have been created by download_dir.mkdir() earlier\n",
    "        print(f\"Output directory {output_dir_path} doesn't exist! Attempting to create.\")\n",
    "        try:\n",
    "            output_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Failed to create output directory {output_dir_path}: {e}\")\n",
    "\n",
    "    target_file = output_dir_path / basename(url) # Use Path object for joining\n",
    "\n",
    "    if target_file.is_file() and not force:\n",
    "        print(f'\\n{target_file.name} already exists, and force=False, not re-downloading.')\n",
    "        return 0 # Return 0 bytes downloaded\n",
    "\n",
    "    # Use the session created by request.install_opener() implicitly with requests.get\n",
    "    # The setup_earthdata_login_auth configures the global urllib.request opener,\n",
    "    # requests library needs its own auth handling if not using the default opener\n",
    "    # or if cookies are essential and managed by urllib.\n",
    "    # For simplicity and to leverage setup_earthdata_login_auth directly with `requests`,\n",
    "    # it's better if setup_earthdata_login_auth directly configures `requests.Session`\n",
    "    # or if download_file uses `urllib.request.urlopen`.\n",
    "    # However, the original code uses requests.get() here, assuming cookies set by\n",
    "    # install_opener() might be picked up or that basic auth is sufficient.\n",
    "    # Let's try to use the installed opener more directly for robustness:\n",
    "    \n",
    "    try:\n",
    "        # We use requests.get as per original code, assuming the session/cookie handling\n",
    "        # from urllib.request setup is somehow bridged or not strictly needed for these direct HTTPS downloads\n",
    "        # once authenticated for the session. If complex cookie handling is needed, this might need\n",
    "        # to use urllib.request.urlopen(url) or a shared requests.Session.\n",
    "        # For now, we keep `requests.get(url, stream=True)` for chunked download,\n",
    "        # assuming authentication is handled for the domain.\n",
    "        with requests.get(url, stream=True, timeout=30) as r: # Added stream=True and timeout\n",
    "            r.raise_for_status() # Raise an exception for HTTP errors\n",
    "            \n",
    "            with open(target_file, 'wb') as f:\n",
    "                total_size_in_bytes = int(r.headers.get('content-length', 0))\n",
    "                # Use shutil.copyfileobj for potentially more efficient streaming\n",
    "                # shutil.copyfileobj(r.raw, f) # Alternative if r.iter_content is problematic\n",
    "                for chunk in r.iter_content(chunk_size=8192): # Increased chunk size\n",
    "                    if chunk: # filter out keep-alive new chunks\n",
    "                        f.write(chunk)\n",
    "            \n",
    "            # Verify file size if content-length was provided\n",
    "            if total_size_in_bytes != 0 and target_file.stat().st_size != total_size_in_bytes:\n",
    "                print(f\"Warning: Downloaded file {target_file.name} size {target_file.stat().st_size} \"\n",
    "                      f\"does not match expected size {total_size_in_bytes}.\")\n",
    "            elif total_size_in_bytes == 0 and target_file.stat().st_size == 0:\n",
    "                 print(f\"Warning: Downloaded file {target_file.name} is empty (0 bytes). URL: {url}\")\n",
    "\n",
    "\n",
    "            return target_file.stat().st_size # Return actual downloaded size\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"\\nHTTP Error downloading {url}: {e}\")\n",
    "        print(f\"Response content: {e.response.text if e.response else 'No response content'}\")\n",
    "        if target_file.is_file(): # Remove partially downloaded file\n",
    "            target_file.unlink(missing_ok=True)\n",
    "        return 0 # Indicate failure or 0 bytes\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"\\nError downloading {url}: {e}\")\n",
    "        if target_file.is_file(): # Remove partially downloaded file\n",
    "            target_file.unlink(missing_ok=True)\n",
    "        return 0 # Indicate failure or 0 bytes\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred downloading {url}: {e}\")\n",
    "        if target_file.is_file(): # Remove partially downloaded file\n",
    "            target_file.unlink(missing_ok=True)\n",
    "        return 0\n",
    "\n",
    "\n",
    "# Function to download all URLs concurrently\n",
    "def download_files_concurrently(dls, download_dir_path: Path, force=False, max_workers=6):\n",
    "    \"\"\"Downloads files concurrently using a thread pool.\"\"\"\n",
    "    start_time = time.time()\n",
    "    results = []\n",
    "\n",
    "    print(f\"\\nStarting concurrent download of {len(dls)} files to {download_dir_path} with {max_workers} workers...\")\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Prepare arguments for executor.map\n",
    "        # The download_file function will be called with (url, download_dir_path, force)\n",
    "        # repeat(download_dir_path) and repeat(force) create iterators that yield the same value\n",
    "        # for each call.\n",
    "        # tqdm wraps the executor.map to provide a progress bar.\n",
    "        results_iterator = executor.map(download_file, dls, repeat(download_dir_path), repeat(force))\n",
    "        \n",
    "        # tqdm processes the iterator and shows progress\n",
    "        # We need to convert the iterator to a list to ensure all tasks are completed\n",
    "        # and exceptions are raised if they occurred in threads.\n",
    "        try:\n",
    "            results = list(tqdm(results_iterator, total=len(dls), desc=\"Downloading files\"))\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during concurrent downloads: {e}\")\n",
    "            # Potentially some downloads succeeded, some failed. Results might be partial.\n",
    "            # The 'results' list will contain exceptions for tasks that failed if they weren't caught inside download_file.\n",
    "\n",
    "    total_download_size_in_bytes = np.sum(np.array(results, dtype=np.int64)) # Ensure int64 for sum\n",
    "    total_time = time.time() - start_time\n",
    "    num_successful_downloads = sum(1 for r in results if isinstance(r, (int, float)) and r > 0)\n",
    "\n",
    "\n",
    "    print('\\n=====================================')\n",
    "    print(f'Download process complete.')\n",
    "    print(f'Successfully downloaded {num_successful_downloads}/{len(dls)} files.')\n",
    "    print(f'Total downloaded data: {np.round(total_download_size_in_bytes / 1e6, 2)} MB')\n",
    "    if total_time > 0:\n",
    "      print(f'Average download speed: {np.round(total_download_size_in_bytes / 1e6 / total_time, 2)} MB/s (for successful downloads)')\n",
    "    else:\n",
    "      print(f'Total time was negligible.')\n",
    "    print('=====================================')\n",
    "    return [d for i, d in enumerate(dls) if isinstance(results[i], (int, float)) and results[i] > 0]\n",
    "\n",
    "\n",
    "# ----- Main Script -----\n",
    "def main():\n",
    "    # OISSS dataset shortname\n",
    "    ShortName = \"OISSS_L4_multimission_monthly_v2\"\n",
    "\n",
    "    # Specify desired date range - adjust as needed\n",
    "    # This dataset is cited to start from August 2011\n",
    "    StartDate = \"2016-01-01\"\n",
    "    EndDate = \"2016-12-31\" # Inclusive end date for the search\n",
    "\n",
    "    # Define download directory using Pathlib\n",
    "    download_root_dir = Path('./Earth_Data')  # Change this for your desired root directory\n",
    "    download_dir = download_root_dir / ShortName\n",
    "\n",
    "    # Create the download directory if it doesn't exist\n",
    "    download_dir.mkdir(exist_ok=True, parents=True)\n",
    "    print(f'Download directory: {download_dir.resolve()}')\n",
    "\n",
    "    # Log into Earthdata - this sets up global authentication for urllib.request\n",
    "    # `requests` might need explicit auth or session handling if cookies are critical\n",
    "    # and not bridged from urllib's global state.\n",
    "    setup_earthdata_login_auth()\n",
    "\n",
    "    # Search parameters for the dataset\n",
    "    input_search_params = {\n",
    "        'ShortName': ShortName,\n",
    "        'temporal': f\"{StartDate},{EndDate}\" # CMR expects temporal as a comma-separated string\n",
    "    }\n",
    "\n",
    "    print(f\"\\nSearching CMR for {ShortName} data between {StartDate} and {EndDate}...\")\n",
    "\n",
    "    try:\n",
    "        grans_df = get_granules(input_search_params)\n",
    "        \n",
    "        if grans_df.empty or 'Online Access URLs' not in grans_df.columns:\n",
    "            print(\"No granules found or 'Online Access URLs' column missing in CMR response.\")\n",
    "            return\n",
    "\n",
    "        # Filter out rows where 'Online Access URLs' might be NaN or empty strings if any\n",
    "        grans_df.dropna(subset=['Online Access URLs'], inplace=True)\n",
    "        dls = grans_df['Online Access URLs'].tolist()\n",
    "        num_grans_to_download = len(dls)\n",
    "\n",
    "        print(f'\\nCMR search yielded {num_grans_to_download} granules with downloadable URLs.')\n",
    "\n",
    "        if num_grans_to_download == 0:\n",
    "            print(\"No files to download.\")\n",
    "            return\n",
    "\n",
    "        print(\"\\nURLs to be downloaded:\")\n",
    "        for i, url in enumerate(dls):\n",
    "            print(f\"{i+1}: {url}\")\n",
    "        \n",
    "        # Expected number of months (for a full year)\n",
    "        start_month = pd.to_datetime(StartDate).month\n",
    "        end_month = pd.to_datetime(EndDate).month\n",
    "        start_year = pd.to_datetime(StartDate).year\n",
    "        end_year = pd.to_datetime(EndDate).year\n",
    "        \n",
    "        expected_months = 0\n",
    "        if start_year == end_year:\n",
    "            expected_months = (end_month - start_month + 1)\n",
    "        else: # Multi-year, more complex, but for 2016-01-01 to 2016-12-31 it's 12\n",
    "            expected_months = (12 - start_month + 1) + (end_year - start_year - 1) * 12 + end_month\n",
    "        \n",
    "        print(f\"Expected number of monthly granules for the period: {expected_months}\")\n",
    "        if num_grans_to_download < expected_months:\n",
    "            print(f\"Warning: Found fewer granules ({num_grans_to_download}) than expected ({expected_months}). Some data may be missing from the source for this period.\")\n",
    "\n",
    "\n",
    "        # Download the granules with concurrent downloads\n",
    "        max_workers = 4  # Adjust based on your internet connection and server limits (too many can cause issues)\n",
    "        force_redownload = False # Set to True to force redownload of existing files\n",
    "\n",
    "        print(f\"\\nProceeding to download {num_grans_to_download} files. Force redownload: {force_redownload}\")\n",
    "        successfully_downloaded_urls = download_files_concurrently(dls, download_dir, force_redownload, max_workers)\n",
    "\n",
    "        # List downloaded files (based on successful downloads reported by the function)\n",
    "        # This is more accurate than globbing if some downloads failed.\n",
    "        print(f'\\nSuccessfully downloaded {len(successfully_downloaded_urls)} files:')\n",
    "        # For verification, list files in the directory as well\n",
    "        if download_dir.exists():\n",
    "            print(f\"\\nFiles found in download directory ({download_dir}):\")\n",
    "            downloaded_files_in_dir = sorted(list(download_dir.glob('*.nc')))\n",
    "            for f_path in downloaded_files_in_dir:\n",
    "                print(f_path.name)\n",
    "            print(f\"Total .nc files in directory: {len(downloaded_files_in_dir)}\")\n",
    "        \n",
    "            if len(downloaded_files_in_dir) < expected_months and len(downloaded_files_in_dir) == len(successfully_downloaded_urls):\n",
    "                 print(\"\\n--- IMPORTANT ---\")\n",
    "                 print(\"If the number of downloaded files is less than expected, \"\n",
    "                       \"it's highly likely that the data for the missing months was not found by the CMR search.\")\n",
    "                 print(\"This means the data might not be available from NASA for your specified product and date range.\")\n",
    "                 print(\"You can verify this by manually searching on the Earthdata Search portal:\")\n",
    "                 print(\"https://search.earthdata.nasa.gov/search\")\n",
    "                 print(f\"using ShortName: {ShortName} and dates: {StartDate} to {EndDate}\")\n",
    "                 print(\"---\")\n",
    "\n",
    "\n",
    "        # Optionally, open and plot data (example)\n",
    "        if len(successfully_downloaded_urls) > 0:\n",
    "            print(\"\\nExample of how you can open the files for analysis (if they are NetCDF):\")\n",
    "            # Construct file paths for successfully downloaded NetCDF files\n",
    "            salinity_nc_files = [download_dir / basename(url) for url in successfully_downloaded_urls if url.endswith('.nc')]\n",
    "            if salinity_nc_files:\n",
    "                # Ensure files actually exist before trying to open\n",
    "                salinity_nc_files_exist = [f for f in salinity_nc_files if f.is_file()]\n",
    "                if salinity_nc_files_exist:\n",
    "                    print(\"xds = xr.open_mfdataset(salinity_files, engine='netcdf4', parallel=True, combine='by_coords')\")\n",
    "                    print(\"# Example plot, actual variable name might differ, check your .nc file structure\")\n",
    "                    print(\"# xds['your_salinity_variable_name'].mean('time').plot(figsize=[12,6])\")\n",
    "                else:\n",
    "                    print(\"No successfully downloaded .nc files found to demonstrate opening.\")\n",
    "            else:\n",
    "                print(\"No .nc files were in the list of successfully downloaded URLs.\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred in the main script: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b9ba35-eb5b-483f-a0ef-453b787fae50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
