{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba392a80-e4fc-4923-9720-d6121379d950",
   "metadata": {},
   "source": [
    "Working Monthly AQUA MODIS CHL merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd951cb6-7292-4d70-91be-a86e9521769a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test filename: AQUA_MODIS.20150501_20150531.L3m.MO.CHL.x_chlor_a.nc\n",
      "Extracted date: 2015-05-01 00:00:00\n",
      "Expected: 2015-05-01\n",
      "Found 121 files to combine\n",
      "Estimated total size: 1.3 GB\n",
      "Using simple concatenation...\n",
      "Processing file 1/121\n",
      "Processing file 101/121\n",
      "Concatenating datasets...\n",
      "Saving to CHL_MO_subset_combined.nc4...\n",
      "✅ Successfully created CHL_MO_subset_combined.nc4\n",
      "Time range: 2015-05-01T00:00:00.000000000 to 2025-05-01T00:00:00.000000000\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_date_from_filename(filename):\n",
    "    \"\"\"Extract the first date from filename format like 'AQUA_MODIS.20150501_20150531.L3m.MO.CHL.x_chlor_a.nc'\"\"\"\n",
    "    basename = os.path.basename(filename)\n",
    "    parts = basename.split('.')\n",
    "    \n",
    "    # Look for the date range part (e.g., \"20150501_20150531\")\n",
    "    for part in parts:\n",
    "        if '_' in part and len(part) == 17:  # Format: YYYYMMDD_YYYYMMDD\n",
    "            date_parts = part.split('_')\n",
    "            if len(date_parts) == 2 and len(date_parts[0]) == 8 and date_parts[0].isdigit():\n",
    "                try:\n",
    "                    # Return the first date\n",
    "                    return pd.to_datetime(date_parts[0], format='%Y%m%d')\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    # Fallback: look for any 8-digit date\n",
    "    for part in basename.split('_'):\n",
    "        if len(part) == 8 and part.isdigit():\n",
    "            try:\n",
    "                return pd.to_datetime(part, format='%Y%m%d')\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    return None\n",
    "\n",
    "def combine_netcdf_files(folder_path, output_file):\n",
    "    \"\"\"\n",
    "    Simple function to combine all NetCDF files in a folder\n",
    "    \"\"\"\n",
    "    # Find all files\n",
    "    file_pattern = os.path.join(folder_path, \"*AQUA_MODIS.*x_chlor_a.nc*\")\n",
    "    files = sorted(glob.glob(file_pattern))\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"No files found matching pattern in {folder_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(files)} files to combine\")\n",
    "    \n",
    "    # Quick file size check\n",
    "    sample_size = sum(os.path.getsize(f) for f in files[:5]) / 5\n",
    "    total_gb = (sample_size * len(files)) / (1024**3)\n",
    "    print(f\"Estimated total size: {total_gb:.1f} GB\")\n",
    "    \n",
    "    if total_gb > 20:\n",
    "        print(\"Large dataset detected - using chunked processing...\")\n",
    "        return combine_large_dataset(files, output_file)\n",
    "    else:\n",
    "        print(\"Using simple concatenation...\")\n",
    "        return combine_small_dataset(files, output_file)\n",
    "\n",
    "def combine_small_dataset(files, output_file):\n",
    "    \"\"\"For smaller datasets - load everything into memory\"\"\"\n",
    "    datasets = []\n",
    "    \n",
    "    for i, file_path in enumerate(files):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processing file {i+1}/{len(files)}\")\n",
    "        \n",
    "        try:\n",
    "            ds = xr.open_dataset(file_path)\n",
    "            \n",
    "            # Add time coordinate\n",
    "            date = extract_date_from_filename(file_path)\n",
    "            if date:\n",
    "                ds = ds.expand_dims('time')\n",
    "                ds = ds.assign_coords(time=[date])\n",
    "                datasets.append(ds)\n",
    "            else:\n",
    "                print(f\"Warning: Could not extract date from {file_path}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not datasets:\n",
    "        print(\"No valid datasets found!\")\n",
    "        return\n",
    "    \n",
    "    # Combine all datasets\n",
    "    print(\"Concatenating datasets...\")\n",
    "    combined = xr.concat(datasets, dim='time')\n",
    "    combined = combined.sortby('time')\n",
    "    \n",
    "    # Add metadata\n",
    "    combined.attrs['created_from'] = f\"{len(datasets)} files\"\n",
    "    combined.attrs['creation_date'] = pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # Save with compression\n",
    "    print(f\"Saving to {output_file}...\")\n",
    "    encoding = {var: {'zlib': True, 'complevel': 1} for var in combined.data_vars}\n",
    "    combined.to_netcdf(output_file, encoding=encoding)\n",
    "    \n",
    "    # Clean up\n",
    "    for ds in datasets:\n",
    "        ds.close()\n",
    "    combined.close()\n",
    "    \n",
    "    print(f\" Successfully created {output_file}\")\n",
    "    print(f\"Time range: {combined.time.min().values} to {combined.time.max().values}\")\n",
    "\n",
    "def combine_large_dataset(files, output_file):\n",
    "    \"\"\"For larger datasets - use xarray's built-in chunking\"\"\"\n",
    "    print(\"Using xarray open_mfdataset for large dataset...\")\n",
    "    \n",
    "    # Use xarray's built-in multi-file dataset opener\n",
    "    def preprocess(ds):\n",
    "        \"\"\"Add time coordinate during preprocessing\"\"\"\n",
    "        # This is tricky because we don't have filename in preprocess\n",
    "        # So we'll use a different approach\n",
    "        return ds\n",
    "    \n",
    "    # Alternative approach for large datasets\n",
    "    temp_files = []\n",
    "    batch_size = 500  # Process in smaller batches\n",
    "    \n",
    "    for i in range(0, len(files), batch_size):\n",
    "        batch = files[i:i + batch_size]\n",
    "        print(f\"Processing batch {i//batch_size + 1}/{(len(files)-1)//batch_size + 1}\")\n",
    "        \n",
    "        batch_datasets = []\n",
    "        for file_path in batch:\n",
    "            try:\n",
    "                ds = xr.open_dataset(file_path)\n",
    "                date = extract_date_from_filename(file_path)\n",
    "                if date:\n",
    "                    ds = ds.expand_dims('time')\n",
    "                    ds = ds.assign_coords(time=[date])\n",
    "                    batch_datasets.append(ds)\n",
    "            except Exception as e:\n",
    "                print(f\"Error with {file_path}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if batch_datasets:\n",
    "            batch_combined = xr.concat(batch_datasets, dim='time')\n",
    "            temp_file = f\"temp_batch_{i//batch_size}.nc4\"\n",
    "            batch_combined.to_netcdf(temp_file)\n",
    "            temp_files.append(temp_file)\n",
    "            \n",
    "            # Clean up\n",
    "            batch_combined.close()\n",
    "            for ds in batch_datasets:\n",
    "                ds.close()\n",
    "    \n",
    "    # Combine all temporary files\n",
    "    print(\"Combining all batches...\")\n",
    "    if temp_files:\n",
    "        final_ds = xr.open_mfdataset(temp_files, concat_dim='time', combine='nested')\n",
    "        final_ds = final_ds.sortby('time')\n",
    "        \n",
    "        # Save final result\n",
    "        encoding = {var: {'zlib': True, 'complevel': 1} for var in final_ds.data_vars}\n",
    "        final_ds.to_netcdf(output_file, encoding=encoding)\n",
    "        final_ds.close()\n",
    "        \n",
    "        # Clean up temp files\n",
    "        for temp_file in temp_files:\n",
    "            os.remove(temp_file)\n",
    "        \n",
    "        print(f\"✅ Successfully created {output_file}\")\n",
    "\n",
    "# Test the date extraction function\n",
    "def test_date_extraction():\n",
    "    \"\"\"Test function to verify date extraction works correctly\"\"\"\n",
    "    test_filename = \"AQUA_MODIS.20150501_20150531.L3m.MO.CHL.x_chlor_a.nc\"\n",
    "    extracted_date = extract_date_from_filename(test_filename)\n",
    "    print(f\"Test filename: {test_filename}\")\n",
    "    print(f\"Extracted date: {extracted_date}\")\n",
    "    print(f\"Expected: 2015-05-01\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test the date extraction\n",
    "    test_date_extraction()\n",
    "    \n",
    "    # Simple usage\n",
    "    folder_path = \"CHL_MO_subset_v2\"\n",
    "    output_file = \"CHL_MO_subset_combined.nc4\"\n",
    "    \n",
    "    combine_netcdf_files(folder_path, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a141a0e-31b2-4e42-a34c-23c4cb73add6",
   "metadata": {},
   "source": [
    "Working 8Day AQUA MODIS CHL merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df319c98-a9a7-40b3-b570-07eb564c190d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test filename: AQUA_MODIS.20150501_20150508.L3m.8D.CHL.x_chlor_a.nc\n",
      "Extracted date: 2015-05-01 00:00:00\n",
      "Expected: 2015-05-01\n",
      "Found 463 files to combine\n",
      "Estimated total size: 3.7 GB\n",
      "Using simple concatenation...\n",
      "Processing file 1/463\n",
      "Processing file 101/463\n",
      "Processing file 201/463\n",
      "Processing file 301/463\n",
      "Processing file 401/463\n",
      "Concatenating datasets...\n",
      "Saving to CHL_8D_subset_combined.nc4...\n",
      "✅ Successfully created CHL_8D_subset_combined.nc4\n",
      "Time range: 2015-05-01T00:00:00.000000000 to 2025-05-25T00:00:00.000000000\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_date_from_filename(filename):\n",
    "    \"\"\"Extract the first date from filename format like 'AQUA_MODIS.20150501_20150508.L3m.8D.CHL.x_chlor_a.nc'\"\"\"\n",
    "    basename = os.path.basename(filename)\n",
    "    parts = basename.split('.')\n",
    "    \n",
    "    # Look for the date range part (e.g., \"20150501_20150531\")\n",
    "    for part in parts:\n",
    "        if '_' in part and len(part) == 17:  # Format: YYYYMMDD_YYYYMMDD\n",
    "            date_parts = part.split('_')\n",
    "            if len(date_parts) == 2 and len(date_parts[0]) == 8 and date_parts[0].isdigit():\n",
    "                try:\n",
    "                    # Return the first date\n",
    "                    return pd.to_datetime(date_parts[0], format='%Y%m%d')\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    # Fallback: look for any 8-digit date\n",
    "    for part in basename.split('_'):\n",
    "        if len(part) == 8 and part.isdigit():\n",
    "            try:\n",
    "                return pd.to_datetime(part, format='%Y%m%d')\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    return None\n",
    "\n",
    "def combine_netcdf_files(folder_path, output_file):\n",
    "    \"\"\"\n",
    "    Simple function to combine all NetCDF files in a folder\n",
    "    \"\"\"\n",
    "    # Find all files\n",
    "    file_pattern = os.path.join(folder_path, \"*AQUA_MODIS.*x_chlor_a.nc*\")\n",
    "    files = sorted(glob.glob(file_pattern))\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"No files found matching pattern in {folder_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(files)} files to combine\")\n",
    "    \n",
    "    # Quick file size check\n",
    "    sample_size = sum(os.path.getsize(f) for f in files[:5]) / 5\n",
    "    total_gb = (sample_size * len(files)) / (1024**3)\n",
    "    print(f\"Estimated total size: {total_gb:.1f} GB\")\n",
    "    \n",
    "    if total_gb > 20:\n",
    "        print(\"Large dataset detected - using chunked processing...\")\n",
    "        return combine_large_dataset(files, output_file)\n",
    "    else:\n",
    "        print(\"Using simple concatenation...\")\n",
    "        return combine_small_dataset(files, output_file)\n",
    "\n",
    "def combine_small_dataset(files, output_file):\n",
    "    \"\"\"For smaller datasets - load everything into memory\"\"\"\n",
    "    datasets = []\n",
    "    \n",
    "    for i, file_path in enumerate(files):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processing file {i+1}/{len(files)}\")\n",
    "        \n",
    "        try:\n",
    "            ds = xr.open_dataset(file_path)\n",
    "            \n",
    "            # Add time coordinate\n",
    "            date = extract_date_from_filename(file_path)\n",
    "            if date:\n",
    "                ds = ds.expand_dims('time')\n",
    "                ds = ds.assign_coords(time=[date])\n",
    "                datasets.append(ds)\n",
    "            else:\n",
    "                print(f\"Warning: Could not extract date from {file_path}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not datasets:\n",
    "        print(\"No valid datasets found!\")\n",
    "        return\n",
    "    \n",
    "    # Combine all datasets\n",
    "    print(\"Concatenating datasets...\")\n",
    "    combined = xr.concat(datasets, dim='time')\n",
    "    combined = combined.sortby('time')\n",
    "    \n",
    "    # Add metadata\n",
    "    combined.attrs['created_from'] = f\"{len(datasets)} files\"\n",
    "    combined.attrs['creation_date'] = pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # Save with compression\n",
    "    print(f\"Saving to {output_file}...\")\n",
    "    encoding = {var: {'zlib': True, 'complevel': 1} for var in combined.data_vars}\n",
    "    combined.to_netcdf(output_file, encoding=encoding)\n",
    "    \n",
    "    # Clean up\n",
    "    for ds in datasets:\n",
    "        ds.close()\n",
    "    combined.close()\n",
    "    \n",
    "    print(f\"✅ Successfully created {output_file}\")\n",
    "    print(f\"Time range: {combined.time.min().values} to {combined.time.max().values}\")\n",
    "\n",
    "def combine_large_dataset(files, output_file):\n",
    "    \"\"\"For larger datasets - use xarray's built-in chunking\"\"\"\n",
    "    print(\"Using xarray open_mfdataset for large dataset...\")\n",
    "    \n",
    "    # Use xarray's built-in multi-file dataset opener\n",
    "    def preprocess(ds):\n",
    "        \"\"\"Add time coordinate during preprocessing\"\"\"\n",
    "        # This is tricky because we don't have filename in preprocess\n",
    "        # So we'll use a different approach\n",
    "        return ds\n",
    "    \n",
    "    # Alternative approach for large datasets\n",
    "    temp_files = []\n",
    "    batch_size = 500  # Process in smaller batches\n",
    "    \n",
    "    for i in range(0, len(files), batch_size):\n",
    "        batch = files[i:i + batch_size]\n",
    "        print(f\"Processing batch {i//batch_size + 1}/{(len(files)-1)//batch_size + 1}\")\n",
    "        \n",
    "        batch_datasets = []\n",
    "        for file_path in batch:\n",
    "            try:\n",
    "                ds = xr.open_dataset(file_path)\n",
    "                date = extract_date_from_filename(file_path)\n",
    "                if date:\n",
    "                    ds = ds.expand_dims('time')\n",
    "                    ds = ds.assign_coords(time=[date])\n",
    "                    batch_datasets.append(ds)\n",
    "            except Exception as e:\n",
    "                print(f\"Error with {file_path}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if batch_datasets:\n",
    "            batch_combined = xr.concat(batch_datasets, dim='time')\n",
    "            temp_file = f\"temp_batch_{i//batch_size}.nc4\"\n",
    "            batch_combined.to_netcdf(temp_file)\n",
    "            temp_files.append(temp_file)\n",
    "            \n",
    "            # Clean up\n",
    "            batch_combined.close()\n",
    "            for ds in batch_datasets:\n",
    "                ds.close()\n",
    "    \n",
    "    # Combine all temporary files\n",
    "    print(\"Combining all batches...\")\n",
    "    if temp_files:\n",
    "        final_ds = xr.open_mfdataset(temp_files, concat_dim='time', combine='nested')\n",
    "        final_ds = final_ds.sortby('time')\n",
    "        \n",
    "        # Save final result\n",
    "        encoding = {var: {'zlib': True, 'complevel': 1} for var in final_ds.data_vars}\n",
    "        final_ds.to_netcdf(output_file, encoding=encoding)\n",
    "        final_ds.close()\n",
    "        \n",
    "        # Clean up temp files\n",
    "        for temp_file in temp_files:\n",
    "            os.remove(temp_file)\n",
    "        \n",
    "        print(f\"✅ Successfully created {output_file}\")\n",
    "\n",
    "# Test the date extraction function\n",
    "def test_date_extraction():\n",
    "    \"\"\"Test function to verify date extraction works correctly\"\"\"\n",
    "    test_filename = \"AQUA_MODIS.20150501_20150508.L3m.8D.CHL.x_chlor_a.nc\"\n",
    "    extracted_date = extract_date_from_filename(test_filename)\n",
    "    print(f\"Test filename: {test_filename}\")\n",
    "    print(f\"Extracted date: {extracted_date}\")\n",
    "    print(f\"Expected: 2015-05-01\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test the date extraction\n",
    "    test_date_extraction()\n",
    "    \n",
    "    # Simple usage\n",
    "    folder_path = \"CHL_8D_subset_v2\"\n",
    "    output_file = \"CHL_8D_subset_combined.nc4\"\n",
    "    \n",
    "    combine_netcdf_files(folder_path, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748105af-b8c4-4abf-8451-ec82b1c14a4f",
   "metadata": {},
   "source": [
    "Monthly merge for 2004-2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1a71d53-fba0-4cf2-b17c-544fc234af10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test filename: AQUA_MODIS.20150501_20150531.L3m.MO.CHL.x_chlor_a.nc\n",
      "Extracted date: 2015-05-01 00:00:00\n",
      "Expected: 2015-05-01\n",
      "Found 121 files to combine\n",
      "Estimated total size: 1.3 GB\n",
      "Using simple concatenation...\n",
      "Processing file 1/121\n",
      "Processing file 101/121\n",
      "Concatenating datasets...\n",
      "Saving to CHL_MO_2004_2014_subset_combined.nc4...\n",
      "✅ Successfully created CHL_MO_2004_2014_subset_combined.nc4\n",
      "Time range: 2004-05-01T00:00:00.000000000 to 2014-05-01T00:00:00.000000000\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_date_from_filename(filename):\n",
    "    \"\"\"Extract the first date from filename format like 'AQUA_MODIS.20150501_20150531.L3m.MO.CHL.x_chlor_a.nc'\"\"\"\n",
    "    basename = os.path.basename(filename)\n",
    "    parts = basename.split('.')\n",
    "    \n",
    "    # Look for the date range part (e.g., \"20150501_20150531\")\n",
    "    for part in parts:\n",
    "        if '_' in part and len(part) == 17:  # Format: YYYYMMDD_YYYYMMDD\n",
    "            date_parts = part.split('_')\n",
    "            if len(date_parts) == 2 and len(date_parts[0]) == 8 and date_parts[0].isdigit():\n",
    "                try:\n",
    "                    # Return the first date\n",
    "                    return pd.to_datetime(date_parts[0], format='%Y%m%d')\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    # Fallback: look for any 8-digit date\n",
    "    for part in basename.split('_'):\n",
    "        if len(part) == 8 and part.isdigit():\n",
    "            try:\n",
    "                return pd.to_datetime(part, format='%Y%m%d')\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    return None\n",
    "\n",
    "def combine_netcdf_files(folder_path, output_file):\n",
    "    \"\"\"\n",
    "    Simple function to combine all NetCDF files in a folder\n",
    "    \"\"\"\n",
    "    # Find all files\n",
    "    file_pattern = os.path.join(folder_path, \"*AQUA_MODIS.*x_chlor_a.nc*\")\n",
    "    files = sorted(glob.glob(file_pattern))\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"No files found matching pattern in {folder_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(files)} files to combine\")\n",
    "    \n",
    "    # Quick file size check\n",
    "    sample_size = sum(os.path.getsize(f) for f in files[:5]) / 5\n",
    "    total_gb = (sample_size * len(files)) / (1024**3)\n",
    "    print(f\"Estimated total size: {total_gb:.1f} GB\")\n",
    "    \n",
    "    if total_gb > 20:\n",
    "        print(\"Large dataset detected - using chunked processing...\")\n",
    "        return combine_large_dataset(files, output_file)\n",
    "    else:\n",
    "        print(\"Using simple concatenation...\")\n",
    "        return combine_small_dataset(files, output_file)\n",
    "\n",
    "def combine_small_dataset(files, output_file):\n",
    "    \"\"\"For smaller datasets - load everything into memory\"\"\"\n",
    "    datasets = []\n",
    "    \n",
    "    for i, file_path in enumerate(files):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processing file {i+1}/{len(files)}\")\n",
    "        \n",
    "        try:\n",
    "            ds = xr.open_dataset(file_path)\n",
    "            \n",
    "            # Add time coordinate\n",
    "            date = extract_date_from_filename(file_path)\n",
    "            if date:\n",
    "                ds = ds.expand_dims('time')\n",
    "                ds = ds.assign_coords(time=[date])\n",
    "                datasets.append(ds)\n",
    "            else:\n",
    "                print(f\"Warning: Could not extract date from {file_path}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not datasets:\n",
    "        print(\"No valid datasets found!\")\n",
    "        return\n",
    "    \n",
    "    # Combine all datasets\n",
    "    print(\"Concatenating datasets...\")\n",
    "    combined = xr.concat(datasets, dim='time')\n",
    "    combined = combined.sortby('time')\n",
    "    \n",
    "    # Add metadata\n",
    "    combined.attrs['created_from'] = f\"{len(datasets)} files\"\n",
    "    combined.attrs['creation_date'] = pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # Save with compression\n",
    "    print(f\"Saving to {output_file}...\")\n",
    "    encoding = {var: {'zlib': True, 'complevel': 1} for var in combined.data_vars}\n",
    "    combined.to_netcdf(output_file, encoding=encoding)\n",
    "    \n",
    "    # Clean up\n",
    "    for ds in datasets:\n",
    "        ds.close()\n",
    "    combined.close()\n",
    "    \n",
    "    print(f\" Successfully created {output_file}\")\n",
    "    print(f\"Time range: {combined.time.min().values} to {combined.time.max().values}\")\n",
    "\n",
    "def combine_large_dataset(files, output_file):\n",
    "    \"\"\"For larger datasets - use xarray's built-in chunking\"\"\"\n",
    "    print(\"Using xarray open_mfdataset for large dataset...\")\n",
    "    \n",
    "    # Use xarray's built-in multi-file dataset opener\n",
    "    def preprocess(ds):\n",
    "        \"\"\"Add time coordinate during preprocessing\"\"\"\n",
    "        # This is tricky because we don't have filename in preprocess\n",
    "        # So we'll use a different approach\n",
    "        return ds\n",
    "    \n",
    "    # Alternative approach for large datasets\n",
    "    temp_files = []\n",
    "    batch_size = 500  # Process in smaller batches\n",
    "    \n",
    "    for i in range(0, len(files), batch_size):\n",
    "        batch = files[i:i + batch_size]\n",
    "        print(f\"Processing batch {i//batch_size + 1}/{(len(files)-1)//batch_size + 1}\")\n",
    "        \n",
    "        batch_datasets = []\n",
    "        for file_path in batch:\n",
    "            try:\n",
    "                ds = xr.open_dataset(file_path)\n",
    "                date = extract_date_from_filename(file_path)\n",
    "                if date:\n",
    "                    ds = ds.expand_dims('time')\n",
    "                    ds = ds.assign_coords(time=[date])\n",
    "                    batch_datasets.append(ds)\n",
    "            except Exception as e:\n",
    "                print(f\"Error with {file_path}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if batch_datasets:\n",
    "            batch_combined = xr.concat(batch_datasets, dim='time')\n",
    "            temp_file = f\"temp_batch_{i//batch_size}.nc4\"\n",
    "            batch_combined.to_netcdf(temp_file)\n",
    "            temp_files.append(temp_file)\n",
    "            \n",
    "            # Clean up\n",
    "            batch_combined.close()\n",
    "            for ds in batch_datasets:\n",
    "                ds.close()\n",
    "    \n",
    "    # Combine all temporary files\n",
    "    print(\"Combining all batches...\")\n",
    "    if temp_files:\n",
    "        final_ds = xr.open_mfdataset(temp_files, concat_dim='time', combine='nested')\n",
    "        final_ds = final_ds.sortby('time')\n",
    "        \n",
    "        # Save final result\n",
    "        encoding = {var: {'zlib': True, 'complevel': 1} for var in final_ds.data_vars}\n",
    "        final_ds.to_netcdf(output_file, encoding=encoding)\n",
    "        final_ds.close()\n",
    "        \n",
    "        # Clean up temp files\n",
    "        for temp_file in temp_files:\n",
    "            os.remove(temp_file)\n",
    "        \n",
    "        print(f\"✅ Successfully created {output_file}\")\n",
    "\n",
    "# Test the date extraction function\n",
    "def test_date_extraction():\n",
    "    \"\"\"Test function to verify date extraction works correctly\"\"\"\n",
    "    test_filename = \"AQUA_MODIS.20150501_20150531.L3m.MO.CHL.x_chlor_a.nc\"\n",
    "    extracted_date = extract_date_from_filename(test_filename)\n",
    "    print(f\"Test filename: {test_filename}\")\n",
    "    print(f\"Extracted date: {extracted_date}\")\n",
    "    print(f\"Expected: 2015-05-01\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test the date extraction\n",
    "    test_date_extraction()\n",
    "    \n",
    "    # Simple usage\n",
    "    folder_path = \"CHL_MO_2004_2014_subset\"\n",
    "    output_file = \"CHL_MO_2004_2014_subset_combined.nc4\"\n",
    "    \n",
    "    combine_netcdf_files(folder_path, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01582852-551a-425c-89cf-316b79e70b47",
   "metadata": {},
   "source": [
    "Monthly Merge for Hawaii example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a652870d-79a0-49a9-be41-10b17a77fa8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test filename: AQUA_MODIS.20150501_20150531.L3m.MO.CHL.x_chlor_a.nc\n",
      "Extracted date: 2015-05-01 00:00:00\n",
      "Expected: 2015-05-01\n",
      "Found 126 files to combine\n",
      "Estimated total size: 0.0 GB\n",
      "Using simple concatenation...\n",
      "Processing file 1/126\n",
      "Error reading CHLHawaii_MO\\AQUA_MODIS.20150101_20150131.L3m.MO.CHL.x_chlor_a.nc.aux.xml: did not find a match in any of xarray's currently installed IO backends ['netcdf4', 'h5netcdf', 'scipy', 'gini', 'kerchunk', 'pydap', 'rasterio', 'zarr']. Consider explicitly selecting one of the installed engines via the ``engine`` parameter, or installing additional IO dependencies, see:\n",
      "https://docs.xarray.dev/en/stable/getting-started-guide/installing.html\n",
      "https://docs.xarray.dev/en/stable/user-guide/io.html\n",
      "Processing file 101/126\n",
      "Concatenating datasets...\n",
      "Saving to CHL_MO_Hawaii.nc4...\n",
      "✅ Successfully created CHL_MO_Hawaii.nc4\n",
      "Time range: 2015-01-01T00:00:00.000000000 to 2025-05-01T00:00:00.000000000\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_date_from_filename(filename):\n",
    "    \"\"\"Extract the first date from filename format like 'AQUA_MODIS.20150501_20150531.L3m.MO.CHL.x_chlor_a.nc'\"\"\"\n",
    "    basename = os.path.basename(filename)\n",
    "    parts = basename.split('.')\n",
    "    \n",
    "    # Look for the date range part (e.g., \"20150501_20150531\")\n",
    "    for part in parts:\n",
    "        if '_' in part and len(part) == 17:  # Format: YYYYMMDD_YYYYMMDD\n",
    "            date_parts = part.split('_')\n",
    "            if len(date_parts) == 2 and len(date_parts[0]) == 8 and date_parts[0].isdigit():\n",
    "                try:\n",
    "                    # Return the first date\n",
    "                    return pd.to_datetime(date_parts[0], format='%Y%m%d')\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    # Fallback: look for any 8-digit date\n",
    "    for part in basename.split('_'):\n",
    "        if len(part) == 8 and part.isdigit():\n",
    "            try:\n",
    "                return pd.to_datetime(part, format='%Y%m%d')\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    return None\n",
    "\n",
    "def combine_netcdf_files(folder_path, output_file):\n",
    "    \"\"\"\n",
    "    Simple function to combine all NetCDF files in a folder\n",
    "    \"\"\"\n",
    "    # Find all files\n",
    "    file_pattern = os.path.join(folder_path, \"*AQUA_MODIS.*x_chlor_a.nc*\")\n",
    "    files = sorted(glob.glob(file_pattern))\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"No files found matching pattern in {folder_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(files)} files to combine\")\n",
    "    \n",
    "    # Quick file size check\n",
    "    sample_size = sum(os.path.getsize(f) for f in files[:5]) / 5\n",
    "    total_gb = (sample_size * len(files)) / (1024**3)\n",
    "    print(f\"Estimated total size: {total_gb:.1f} GB\")\n",
    "    \n",
    "    if total_gb > 20:\n",
    "        print(\"Large dataset detected - using chunked processing...\")\n",
    "        return combine_large_dataset(files, output_file)\n",
    "    else:\n",
    "        print(\"Using simple concatenation...\")\n",
    "        return combine_small_dataset(files, output_file)\n",
    "\n",
    "def combine_small_dataset(files, output_file):\n",
    "    \"\"\"For smaller datasets - load everything into memory\"\"\"\n",
    "    datasets = []\n",
    "    \n",
    "    for i, file_path in enumerate(files):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processing file {i+1}/{len(files)}\")\n",
    "        \n",
    "        try:\n",
    "            ds = xr.open_dataset(file_path)\n",
    "            \n",
    "            # Add time coordinate\n",
    "            date = extract_date_from_filename(file_path)\n",
    "            if date:\n",
    "                ds = ds.expand_dims('time')\n",
    "                ds = ds.assign_coords(time=[date])\n",
    "                datasets.append(ds)\n",
    "            else:\n",
    "                print(f\"Warning: Could not extract date from {file_path}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not datasets:\n",
    "        print(\"No valid datasets found!\")\n",
    "        return\n",
    "    \n",
    "    # Combine all datasets\n",
    "    print(\"Concatenating datasets...\")\n",
    "    combined = xr.concat(datasets, dim='time')\n",
    "    combined = combined.sortby('time')\n",
    "    \n",
    "    # Add metadata\n",
    "    combined.attrs['created_from'] = f\"{len(datasets)} files\"\n",
    "    combined.attrs['creation_date'] = pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # Save with compression\n",
    "    print(f\"Saving to {output_file}...\")\n",
    "    encoding = {var: {'zlib': True, 'complevel': 1} for var in combined.data_vars}\n",
    "    combined.to_netcdf(output_file, encoding=encoding)\n",
    "    \n",
    "    # Clean up\n",
    "    for ds in datasets:\n",
    "        ds.close()\n",
    "    combined.close()\n",
    "    \n",
    "    print(f\"✅ Successfully created {output_file}\")\n",
    "    print(f\"Time range: {combined.time.min().values} to {combined.time.max().values}\")\n",
    "\n",
    "def combine_large_dataset(files, output_file):\n",
    "    \"\"\"For larger datasets - use xarray's built-in chunking\"\"\"\n",
    "    print(\"Using xarray open_mfdataset for large dataset...\")\n",
    "    \n",
    "    # Use xarray's built-in multi-file dataset opener\n",
    "    def preprocess(ds):\n",
    "        \"\"\"Add time coordinate during preprocessing\"\"\"\n",
    "        # This is tricky because we don't have filename in preprocess\n",
    "        # So we'll use a different approach\n",
    "        return ds\n",
    "    \n",
    "    # Alternative approach for large datasets\n",
    "    temp_files = []\n",
    "    batch_size = 500  # Process in smaller batches\n",
    "    \n",
    "    for i in range(0, len(files), batch_size):\n",
    "        batch = files[i:i + batch_size]\n",
    "        print(f\"Processing batch {i//batch_size + 1}/{(len(files)-1)//batch_size + 1}\")\n",
    "        \n",
    "        batch_datasets = []\n",
    "        for file_path in batch:\n",
    "            try:\n",
    "                ds = xr.open_dataset(file_path)\n",
    "                date = extract_date_from_filename(file_path)\n",
    "                if date:\n",
    "                    ds = ds.expand_dims('time')\n",
    "                    ds = ds.assign_coords(time=[date])\n",
    "                    batch_datasets.append(ds)\n",
    "            except Exception as e:\n",
    "                print(f\"Error with {file_path}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if batch_datasets:\n",
    "            batch_combined = xr.concat(batch_datasets, dim='time')\n",
    "            temp_file = f\"temp_batch_{i//batch_size}.nc4\"\n",
    "            batch_combined.to_netcdf(temp_file)\n",
    "            temp_files.append(temp_file)\n",
    "            \n",
    "            # Clean up\n",
    "            batch_combined.close()\n",
    "            for ds in batch_datasets:\n",
    "                ds.close()\n",
    "    \n",
    "    # Combine all temporary files\n",
    "    print(\"Combining all batches...\")\n",
    "    if temp_files:\n",
    "        final_ds = xr.open_mfdataset(temp_files, concat_dim='time', combine='nested')\n",
    "        final_ds = final_ds.sortby('time')\n",
    "        \n",
    "        # Save final result\n",
    "        encoding = {var: {'zlib': True, 'complevel': 1} for var in final_ds.data_vars}\n",
    "        final_ds.to_netcdf(output_file, encoding=encoding)\n",
    "        final_ds.close()\n",
    "        \n",
    "        # Clean up temp files\n",
    "        for temp_file in temp_files:\n",
    "            os.remove(temp_file)\n",
    "        \n",
    "        print(f\"✅ Successfully created {output_file}\")\n",
    "\n",
    "# Test the date extraction function\n",
    "def test_date_extraction():\n",
    "    \"\"\"Test function to verify date extraction works correctly\"\"\"\n",
    "    test_filename = \"AQUA_MODIS.20150501_20150531.L3m.MO.CHL.x_chlor_a.nc\"\n",
    "    extracted_date = extract_date_from_filename(test_filename)\n",
    "    print(f\"Test filename: {test_filename}\")\n",
    "    print(f\"Extracted date: {extracted_date}\")\n",
    "    print(f\"Expected: 2015-05-01\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test the date extraction\n",
    "    test_date_extraction()\n",
    "    \n",
    "    # Simple usage\n",
    "    folder_path = \"CHLHawaii_MO\"\n",
    "    output_file = \"CHL_MO_Hawaii.nc4\"\n",
    "    \n",
    "    combine_netcdf_files(folder_path, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b7f912-e191-4263-9231-6119f7486492",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
