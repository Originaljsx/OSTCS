{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "914afc96-4d4d-4758-a96e-d85da192fd68",
   "metadata": {},
   "source": [
    "This worked -final version-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "373ac317-ff46-42f5-a7c6-916c6d661de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß FIXING TIME COORDINATES AND MERGING\n",
      "=============================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Inspect original time coordinates first? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç INSPECTING ORIGINAL TIME COORDINATES\n",
      "==================================================\n",
      "\n",
      "üìÑ File 1: monthly_avg_201505.nc\n",
      "   Time dimension: <xarray.DataArray 'time' (time: 1)> Size: 8B\n",
      "array(['2015-05-15T12:00:00.000000000'], dtype='datetime64[ns]')\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 8B 2015-05-15T12:00:00\n",
      "   Time values: ['2015-05-15T12:00:00.000000000']\n",
      "   Time attrs: {}\n",
      "   Time dtype: datetime64[ns]\n",
      "\n",
      "üìÑ File 2: monthly_avg_201506.nc\n",
      "   Time dimension: <xarray.DataArray 'time' (time: 1)> Size: 8B\n",
      "array(['2015-06-15T12:00:00.000000000'], dtype='datetime64[ns]')\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 8B 2015-06-15T12:00:00\n",
      "   Time values: ['2015-06-15T12:00:00.000000000']\n",
      "   Time attrs: {}\n",
      "   Time dtype: datetime64[ns]\n",
      "\n",
      "üìÑ File 3: monthly_avg_201507.nc\n",
      "   Time dimension: <xarray.DataArray 'time' (time: 1)> Size: 8B\n",
      "array(['2015-07-15T12:00:00.000000000'], dtype='datetime64[ns]')\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 8B 2015-07-15T12:00:00\n",
      "   Time values: ['2015-07-15T12:00:00.000000000']\n",
      "   Time attrs: {}\n",
      "   Time dtype: datetime64[ns]\n",
      "\n",
      "üìÑ File 4: monthly_avg_201508.nc\n",
      "   Time dimension: <xarray.DataArray 'time' (time: 1)> Size: 8B\n",
      "array(['2015-08-15T12:00:00.000000000'], dtype='datetime64[ns]')\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 8B 2015-08-15T12:00:00\n",
      "   Time values: ['2015-08-15T12:00:00.000000000']\n",
      "   Time attrs: {}\n",
      "   Time dtype: datetime64[ns]\n",
      "\n",
      "üìÑ File 5: monthly_avg_201509.nc\n",
      "   Time dimension: <xarray.DataArray 'time' (time: 1)> Size: 8B\n",
      "array(['2015-09-15T12:00:00.000000000'], dtype='datetime64[ns]')\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 8B 2015-09-15T12:00:00\n",
      "   Time values: ['2015-09-15T12:00:00.000000000']\n",
      "   Time attrs: {}\n",
      "   Time dtype: datetime64[ns]\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Proceed with fixing time coordinates and merging? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Found 121 monthly files\n",
      "Processing monthly_avg_201505.nc: 201505 -> 2015-05-01\n",
      "Processing monthly_avg_201506.nc: 201506 -> 2015-06-01\n",
      "Processing monthly_avg_201507.nc: 201507 -> 2015-07-01\n",
      "Processing monthly_avg_201508.nc: 201508 -> 2015-08-01\n",
      "Processing monthly_avg_201509.nc: 201509 -> 2015-09-01\n",
      "Processing monthly_avg_201510.nc: 201510 -> 2015-10-01\n",
      "Processing monthly_avg_201511.nc: 201511 -> 2015-11-01\n",
      "Processing monthly_avg_201512.nc: 201512 -> 2015-12-01\n",
      "Processing monthly_avg_201601.nc: 201601 -> 2016-01-01\n",
      "Processing monthly_avg_201602.nc: 201602 -> 2016-02-01\n",
      "Processing monthly_avg_201603.nc: 201603 -> 2016-03-01\n",
      "Processing monthly_avg_201604.nc: 201604 -> 2016-04-01\n",
      "Processing monthly_avg_201605.nc: 201605 -> 2016-05-01\n",
      "Processing monthly_avg_201606.nc: 201606 -> 2016-06-01\n",
      "Processing monthly_avg_201607.nc: 201607 -> 2016-07-01\n",
      "Processing monthly_avg_201608.nc: 201608 -> 2016-08-01\n",
      "Processing monthly_avg_201609.nc: 201609 -> 2016-09-01\n",
      "Processing monthly_avg_201610.nc: 201610 -> 2016-10-01\n",
      "Processing monthly_avg_201611.nc: 201611 -> 2016-11-01\n",
      "Processing monthly_avg_201612.nc: 201612 -> 2016-12-01\n",
      "Processing monthly_avg_201701.nc: 201701 -> 2017-01-01\n",
      "Processing monthly_avg_201702.nc: 201702 -> 2017-02-01\n",
      "Processing monthly_avg_201703.nc: 201703 -> 2017-03-01\n",
      "Processing monthly_avg_201704.nc: 201704 -> 2017-04-01\n",
      "Processing monthly_avg_201705.nc: 201705 -> 2017-05-01\n",
      "Processing monthly_avg_201706.nc: 201706 -> 2017-06-01\n",
      "Processing monthly_avg_201707.nc: 201707 -> 2017-07-01\n",
      "Processing monthly_avg_201708.nc: 201708 -> 2017-08-01\n",
      "Processing monthly_avg_201709.nc: 201709 -> 2017-09-01\n",
      "Processing monthly_avg_201710.nc: 201710 -> 2017-10-01\n",
      "Processing monthly_avg_201711.nc: 201711 -> 2017-11-01\n",
      "Processing monthly_avg_201712.nc: 201712 -> 2017-12-01\n",
      "Processing monthly_avg_201801.nc: 201801 -> 2018-01-01\n",
      "Processing monthly_avg_201802.nc: 201802 -> 2018-02-01\n",
      "Processing monthly_avg_201803.nc: 201803 -> 2018-03-01\n",
      "Processing monthly_avg_201804.nc: 201804 -> 2018-04-01\n",
      "Processing monthly_avg_201805.nc: 201805 -> 2018-05-01\n",
      "Processing monthly_avg_201806.nc: 201806 -> 2018-06-01\n",
      "Processing monthly_avg_201807.nc: 201807 -> 2018-07-01\n",
      "Processing monthly_avg_201808.nc: 201808 -> 2018-08-01\n",
      "Processing monthly_avg_201809.nc: 201809 -> 2018-09-01\n",
      "Processing monthly_avg_201810.nc: 201810 -> 2018-10-01\n",
      "Processing monthly_avg_201811.nc: 201811 -> 2018-11-01\n",
      "Processing monthly_avg_201812.nc: 201812 -> 2018-12-01\n",
      "Processing monthly_avg_201901.nc: 201901 -> 2019-01-01\n",
      "Processing monthly_avg_201902.nc: 201902 -> 2019-02-01\n",
      "Processing monthly_avg_201903.nc: 201903 -> 2019-03-01\n",
      "Processing monthly_avg_201904.nc: 201904 -> 2019-04-01\n",
      "Processing monthly_avg_201905.nc: 201905 -> 2019-05-01\n",
      "Processing monthly_avg_201906.nc: 201906 -> 2019-06-01\n",
      "Processing monthly_avg_201907.nc: 201907 -> 2019-07-01\n",
      "Processing monthly_avg_201908.nc: 201908 -> 2019-08-01\n",
      "Processing monthly_avg_201909.nc: 201909 -> 2019-09-01\n",
      "Processing monthly_avg_201910.nc: 201910 -> 2019-10-01\n",
      "Processing monthly_avg_201911.nc: 201911 -> 2019-11-01\n",
      "Processing monthly_avg_201912.nc: 201912 -> 2019-12-01\n",
      "Processing monthly_avg_202001.nc: 202001 -> 2020-01-01\n",
      "Processing monthly_avg_202002.nc: 202002 -> 2020-02-01\n",
      "Processing monthly_avg_202003.nc: 202003 -> 2020-03-01\n",
      "Processing monthly_avg_202004.nc: 202004 -> 2020-04-01\n",
      "Processing monthly_avg_202005.nc: 202005 -> 2020-05-01\n",
      "Processing monthly_avg_202006.nc: 202006 -> 2020-06-01\n",
      "Processing monthly_avg_202007.nc: 202007 -> 2020-07-01\n",
      "Processing monthly_avg_202008.nc: 202008 -> 2020-08-01\n",
      "Processing monthly_avg_202009.nc: 202009 -> 2020-09-01\n",
      "Processing monthly_avg_202010.nc: 202010 -> 2020-10-01\n",
      "Processing monthly_avg_202011.nc: 202011 -> 2020-11-01\n",
      "Processing monthly_avg_202012.nc: 202012 -> 2020-12-01\n",
      "Processing monthly_avg_202101.nc: 202101 -> 2021-01-01\n",
      "Processing monthly_avg_202102.nc: 202102 -> 2021-02-01\n",
      "Processing monthly_avg_202103.nc: 202103 -> 2021-03-01\n",
      "Processing monthly_avg_202104.nc: 202104 -> 2021-04-01\n",
      "Processing monthly_avg_202105.nc: 202105 -> 2021-05-01\n",
      "Processing monthly_avg_202106.nc: 202106 -> 2021-06-01\n",
      "Processing monthly_avg_202107.nc: 202107 -> 2021-07-01\n",
      "Processing monthly_avg_202108.nc: 202108 -> 2021-08-01\n",
      "Processing monthly_avg_202109.nc: 202109 -> 2021-09-01\n",
      "Processing monthly_avg_202110.nc: 202110 -> 2021-10-01\n",
      "Processing monthly_avg_202111.nc: 202111 -> 2021-11-01\n",
      "Processing monthly_avg_202112.nc: 202112 -> 2021-12-01\n",
      "Processing monthly_avg_202201.nc: 202201 -> 2022-01-01\n",
      "Processing monthly_avg_202202.nc: 202202 -> 2022-02-01\n",
      "Processing monthly_avg_202203.nc: 202203 -> 2022-03-01\n",
      "Processing monthly_avg_202204.nc: 202204 -> 2022-04-01\n",
      "Processing monthly_avg_202205.nc: 202205 -> 2022-05-01\n",
      "Processing monthly_avg_202206.nc: 202206 -> 2022-06-01\n",
      "Processing monthly_avg_202207.nc: 202207 -> 2022-07-01\n",
      "Processing monthly_avg_202208.nc: 202208 -> 2022-08-01\n",
      "Processing monthly_avg_202209.nc: 202209 -> 2022-09-01\n",
      "Processing monthly_avg_202210.nc: 202210 -> 2022-10-01\n",
      "Processing monthly_avg_202211.nc: 202211 -> 2022-11-01\n",
      "Processing monthly_avg_202212.nc: 202212 -> 2022-12-01\n",
      "Processing monthly_avg_202301.nc: 202301 -> 2023-01-01\n",
      "Processing monthly_avg_202302.nc: 202302 -> 2023-02-01\n",
      "Processing monthly_avg_202303.nc: 202303 -> 2023-03-01\n",
      "Processing monthly_avg_202304.nc: 202304 -> 2023-04-01\n",
      "Processing monthly_avg_202305.nc: 202305 -> 2023-05-01\n",
      "Processing monthly_avg_202306.nc: 202306 -> 2023-06-01\n",
      "Processing monthly_avg_202307.nc: 202307 -> 2023-07-01\n",
      "Processing monthly_avg_202308.nc: 202308 -> 2023-08-01\n",
      "Processing monthly_avg_202309.nc: 202309 -> 2023-09-01\n",
      "Processing monthly_avg_202310.nc: 202310 -> 2023-10-01\n",
      "Processing monthly_avg_202311.nc: 202311 -> 2023-11-01\n",
      "Processing monthly_avg_202312.nc: 202312 -> 2023-12-01\n",
      "Processing monthly_avg_202401.nc: 202401 -> 2024-01-01\n",
      "Processing monthly_avg_202402.nc: 202402 -> 2024-02-01\n",
      "Processing monthly_avg_202403.nc: 202403 -> 2024-03-01\n",
      "Processing monthly_avg_202404.nc: 202404 -> 2024-04-01\n",
      "Processing monthly_avg_202405.nc: 202405 -> 2024-05-01\n",
      "Processing monthly_avg_202406.nc: 202406 -> 2024-06-01\n",
      "Processing monthly_avg_202407.nc: 202407 -> 2024-07-01\n",
      "Processing monthly_avg_202408.nc: 202408 -> 2024-08-01\n",
      "Processing monthly_avg_202409.nc: 202409 -> 2024-09-01\n",
      "Processing monthly_avg_202410.nc: 202410 -> 2024-10-01\n",
      "Processing monthly_avg_202411.nc: 202411 -> 2024-11-01\n",
      "Processing monthly_avg_202412.nc: 202412 -> 2024-12-01\n",
      "Processing monthly_avg_202501.nc: 202501 -> 2025-01-01\n",
      "Processing monthly_avg_202502.nc: 202502 -> 2025-02-01\n",
      "Processing monthly_avg_202503.nc: 202503 -> 2025-03-01\n",
      "Processing monthly_avg_202504.nc: 202504 -> 2025-04-01\n",
      "Processing monthly_avg_202505.nc: 202505 -> 2025-05-01\n",
      "\n",
      "üîó Concatenating 121 datasets...\n",
      "‚úÖ Concatenation successful!\n",
      "üìä Final dimensions: FrozenMappingWarningOnValuesAccess({'time': 121, 'lat': 1704, 'lon': 2602})\n",
      "üïê Time range: 2015-05-01T00:00:00.000000000 to 2025-05-01T00:00:00.000000000\n",
      "üíæ Saving to merged_OSTIA_monthly_2015_2025.nc...\n",
      "\n",
      "‚úÖ SUCCESS!\n",
      "‚è±Ô∏è  Time: 0:03:12.724706\n",
      "üìÅ File: merged_OSTIA_monthly_2015_2025.nc\n",
      "üìä Size: 1.13 GB\n",
      "\n",
      "üîç FINAL VERIFICATION:\n",
      "   Time steps: 121\n",
      "   Date range: 2015-05-01T00:00:00.000000000 to 2025-05-01T00:00:00.000000000\n",
      "   Variables: ['analysed_sst', 'analysis_error', 'sea_ice_fraction', 'mask']\n",
      "   Dimensions: FrozenMappingWarningOnValuesAccess({'time': 121, 'lat': 1704, 'lon': 2602})\n",
      "   First 5 times: ['2015-05-01T00:00:00.000000000' '2015-06-01T00:00:00.000000000'\n",
      " '2015-07-01T00:00:00.000000000' '2015-08-01T00:00:00.000000000'\n",
      " '2015-09-01T00:00:00.000000000']\n",
      "   Last 5 times: ['2025-01-01T00:00:00.000000000' '2025-02-01T00:00:00.000000000'\n",
      " '2025-03-01T00:00:00.000000000' '2025-04-01T00:00:00.000000000'\n",
      " '2025-05-01T00:00:00.000000000']\n",
      "‚úÖ File is valid and accessible!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def fix_monthly_time_coords():\n",
    "    \"\"\"Fix time coordinates in monthly files and merge\"\"\"\n",
    "    \n",
    "    monthly_files = sorted(glob.glob(\"monthly_averages/monthly_avg_*.nc\"))\n",
    "    print(f\"üìÅ Found {len(monthly_files)} monthly files\")\n",
    "    \n",
    "    if not monthly_files:\n",
    "        print(\"‚ùå No monthly files found!\")\n",
    "        return False\n",
    "    \n",
    "    datasets = []\n",
    "    \n",
    "    for file in monthly_files:\n",
    "        # Extract year-month from filename\n",
    "        filename = os.path.basename(file)\n",
    "        year_month = filename.split('_')[2].replace('.nc', '')  # monthly_avg_201505.nc -> 201505\n",
    "        \n",
    "        year = int(year_month[:4])\n",
    "        month = int(year_month[4:6])\n",
    "        \n",
    "        # Create new time coordinate for first day of month\n",
    "        new_time = pd.Timestamp(year, month, 1)\n",
    "        \n",
    "        print(f\"Processing {filename}: {year_month} -> {new_time.strftime('%Y-%m-%d')}\")\n",
    "        \n",
    "        try:\n",
    "            # Open dataset\n",
    "            ds = xr.open_dataset(file)\n",
    "            \n",
    "            # Remove existing time coordinate completely\n",
    "            ds = ds.squeeze('time', drop=True)  # Remove time dimension and coordinate\n",
    "            \n",
    "            # Add new time coordinate\n",
    "            ds = ds.expand_dims('time')\n",
    "            ds['time'] = [new_time]\n",
    "            \n",
    "            # Set time attributes (avoid 'calendar' which conflicts with encoding)\n",
    "            ds.time.attrs = {\n",
    "                'long_name': 'time',\n",
    "                'standard_name': 'time'\n",
    "            }\n",
    "            \n",
    "            datasets.append(ds)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {file}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    print(f\"\\nüîó Concatenating {len(datasets)} datasets...\")\n",
    "    \n",
    "    try:\n",
    "        # Concatenate along time dimension\n",
    "        merged_ds = xr.concat(datasets, dim='time')\n",
    "        \n",
    "        # Sort by time to ensure proper order\n",
    "        merged_ds = merged_ds.sortby('time')\n",
    "        \n",
    "        print(f\"‚úÖ Concatenation successful!\")\n",
    "        print(f\"üìä Final dimensions: {merged_ds.dims}\")\n",
    "        print(f\"üïê Time range: {merged_ds.time.min().values} to {merged_ds.time.max().values}\")\n",
    "        \n",
    "        # Save merged dataset\n",
    "        output_file = \"merged_OSTIA_monthly_2015_2025.nc\"\n",
    "        print(f\"üíæ Saving to {output_file}...\")\n",
    "        \n",
    "        # Encoding with compression and explicit calendar\n",
    "        encoding = {\n",
    "            'time': {'calendar': 'gregorian'}\n",
    "        }\n",
    "        for var in merged_ds.data_vars:\n",
    "            encoding[var] = {\n",
    "                'zlib': True,\n",
    "                'complevel': 6,\n",
    "                'shuffle': True\n",
    "            }\n",
    "        \n",
    "        merged_ds.to_netcdf(output_file, encoding=encoding)\n",
    "        \n",
    "        # Cleanup\n",
    "        for ds in datasets:\n",
    "            ds.close()\n",
    "        merged_ds.close()\n",
    "        \n",
    "        return output_file\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Merge failed: {e}\")\n",
    "        # Cleanup on error\n",
    "        for ds in datasets:\n",
    "            try:\n",
    "                ds.close()\n",
    "            except:\n",
    "                pass\n",
    "        return False\n",
    "\n",
    "def inspect_original_time_coords():\n",
    "    \"\"\"Inspect what's wrong with the original time coordinates\"\"\"\n",
    "    monthly_files = sorted(glob.glob(\"monthly_averages/monthly_avg_*.nc\"))\n",
    "    \n",
    "    print(\"üîç INSPECTING ORIGINAL TIME COORDINATES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, file in enumerate(monthly_files[:5]):  # Check first 5 files\n",
    "        print(f\"\\nüìÑ File {i+1}: {os.path.basename(file)}\")\n",
    "        \n",
    "        try:\n",
    "            ds = xr.open_dataset(file)\n",
    "            \n",
    "            print(f\"   Time dimension: {ds.time}\")\n",
    "            print(f\"   Time values: {ds.time.values}\")\n",
    "            print(f\"   Time attrs: {ds.time.attrs}\")\n",
    "            print(f\"   Time dtype: {ds.time.dtype}\")\n",
    "            \n",
    "            ds.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# Main execution\n",
    "current_dir = os.getcwd()\n",
    "data_dir = os.path.join(current_dir, \"OSTIA_clipped\")\n",
    "\n",
    "print(\"üîß FIXING TIME COORDINATES AND MERGING\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "os.chdir(data_dir)\n",
    "\n",
    "# First, let's see what's wrong with the current time coords\n",
    "response = input(\"Inspect original time coordinates first? (y/n): \")\n",
    "if response.lower() == 'y':\n",
    "    inspect_original_time_coords()\n",
    "\n",
    "# Now fix and merge\n",
    "response = input(\"\\nProceed with fixing time coordinates and merging? (y/n): \")\n",
    "if response.lower() != 'y':\n",
    "    print(\"Cancelled\")\n",
    "    os.chdir(current_dir)\n",
    "    exit(0)\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "result = fix_monthly_time_coords()\n",
    "\n",
    "end_time = datetime.now()\n",
    "duration = end_time - start_time\n",
    "\n",
    "if result:\n",
    "    file_size = os.path.getsize(result) / (1024**3)\n",
    "    print(f\"\\n‚úÖ SUCCESS!\")\n",
    "    print(f\"‚è±Ô∏è  Time: {duration}\")\n",
    "    print(f\"üìÅ File: {result}\")\n",
    "    print(f\"üìä Size: {file_size:.2f} GB\")\n",
    "    \n",
    "    # Final verification\n",
    "    print(f\"\\nüîç FINAL VERIFICATION:\")\n",
    "    try:\n",
    "        test_ds = xr.open_dataset(result)\n",
    "        print(f\"   Time steps: {len(test_ds.time)}\")\n",
    "        print(f\"   Date range: {test_ds.time.min().values} to {test_ds.time.max().values}\")\n",
    "        print(f\"   Variables: {list(test_ds.data_vars.keys())}\")\n",
    "        print(f\"   Dimensions: {test_ds.dims}\")\n",
    "        \n",
    "        # Show first few time values\n",
    "        print(f\"   First 5 times: {test_ds.time[:5].values}\")\n",
    "        print(f\"   Last 5 times: {test_ds.time[-5:].values}\")\n",
    "        \n",
    "        test_ds.close()\n",
    "        print(\"‚úÖ File is valid and accessible!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Verification failed: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(f\"\\n‚ùå FAILED after {duration}\")\n",
    "\n",
    "os.chdir(current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e823d5d3-8f84-4a7e-9381-e8aa9846e037",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
