{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "225c33f8-c7eb-4c63-bdc7-9043824b2b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CREATING MONTHLY AVERAGES ===\n",
      "Loading daily dataset: SMAP_Daily_subset_combined_OISSSfilled.nc4\n",
      "Input dataset info:\n",
      "  Time range: 2015-05-04T00:00:00.000000000 to 2025-06-03T00:00:00.000000000\n",
      "  Time dimension: 3604 daily timesteps\n",
      "  Spatial dimensions: 340 lat Ã— 520 lon\n",
      "  Total data variables: 10\n",
      "\n",
      "Available variables:\n",
      "   1. smap_sss_uncertainty\n",
      "   2. smap_spd\n",
      "   3. ice_fraction\n",
      "   4. anc_sst\n",
      "   5. weight\n",
      "   6. land_fraction\n",
      "   7. smap_high_spd\n",
      "   8. smap_sss\n",
      "   9. anc_sss\n",
      "  10. spatial_ref\n",
      "\n",
      "Variables to be averaged (9):\n",
      "  â€¢ smap_spd (data coverage: 55.6%)\n",
      "  â€¢ smap_sss (data coverage: 55.9%)\n",
      "  â€¢ anc_sss (data coverage: 55.6%)\n",
      "  â€¢ smap_sss_uncertainty (data coverage: 55.6%)\n",
      "  â€¢ weight (data coverage: 99.5%)\n",
      "  â€¢ smap_high_spd (data coverage: 55.6%)\n",
      "  â€¢ land_fraction (data coverage: 55.6%)\n",
      "  â€¢ anc_sst (data coverage: 55.6%)\n",
      "  â€¢ ice_fraction (data coverage: 55.6%)\n",
      "\n",
      "Creating monthly averages...\n",
      "This may take several minutes for large datasets...\n",
      "Creating detailed year-month averages...\n",
      "âœ… Monthly averaging complete!\n",
      "  Original daily data: 3604 timesteps\n",
      "  Monthly averages: 122 months\n",
      "  Time range: 2015-05-01T00:00:00.000000000 to 2025-06-01T00:00:00.000000000\n",
      "\n",
      "Monthly averaging statistics:\n",
      "  smap_spd:\n",
      "    Daily valid points: 354,551,033\n",
      "    Monthly valid points: 12,286,069\n",
      "    Data retention: 3.5%\n",
      "  smap_sss:\n",
      "    Daily valid points: 356,272,716\n",
      "    Monthly valid points: 12,292,386\n",
      "    Data retention: 3.5%\n",
      "  anc_sss:\n",
      "    Daily valid points: 354,551,033\n",
      "    Monthly valid points: 12,286,069\n",
      "    Data retention: 3.5%\n",
      "  smap_sss_uncertainty:\n",
      "    Daily valid points: 354,550,479\n",
      "    Monthly valid points: 12,286,058\n",
      "    Data retention: 3.5%\n",
      "  weight:\n",
      "    Daily valid points: 634,181,600\n",
      "    Monthly valid points: 21,569,600\n",
      "    Data retention: 3.4%\n",
      "  smap_high_spd:\n",
      "    Daily valid points: 354,550,570\n",
      "    Monthly valid points: 12,286,040\n",
      "    Data retention: 3.5%\n",
      "  land_fraction:\n",
      "    Daily valid points: 354,549,945\n",
      "    Monthly valid points: 12,286,068\n",
      "    Data retention: 3.5%\n",
      "  anc_sst:\n",
      "    Daily valid points: 354,551,033\n",
      "    Monthly valid points: 12,286,069\n",
      "    Data retention: 3.5%\n",
      "  ice_fraction:\n",
      "    Daily valid points: 354,551,033\n",
      "    Monthly valid points: 12,286,069\n",
      "    Data retention: 3.5%\n",
      "\n",
      "Saving monthly averages to smap_monthly_means.nc4...\n",
      "âœ… Monthly averages saved: smap_monthly_means.nc4\n",
      "\n",
      "=== VERIFICATION ===\n",
      "Monthly dataset shape: FrozenMappingWarningOnValuesAccess({'time': 122, 'latitude': 340, 'longitude': 520})\n",
      "File size reduction: 29.5x smaller\n",
      "\n",
      "Sample monthly time coordinates:\n",
      "  Month 1: 2015-05-01\n",
      "  Month 2: 2015-06-01\n",
      "  Month 3: 2015-07-01\n",
      "  Month 4: 2015-08-01\n",
      "  Month 5: 2015-09-01\n",
      "  Month 6: 2015-10-01\n",
      "  Month 7: 2015-11-01\n",
      "  Month 8: 2015-12-01\n",
      "  Month 9: 2016-01-01\n",
      "  Month 10: 2016-02-01\n",
      "  ... and 112 more months\n",
      "\n",
      "=== CREATING SEASONAL AVERAGES ===\n",
      "\n",
      "Creating seasonal averages from smap_monthly_means.nc4...\n",
      "âœ… Seasonal averages saved: smap_seasonal_means.nc4\n",
      "\n",
      "ðŸŽ‰ SUCCESS! Files created:\n",
      "   1. smap_monthly_means.nc4 (Monthly averages)\n",
      "   2. smap_seasonal_means.nc4 (Seasonal averages)\n",
      "\n",
      "You can now use these averaged datasets for analysis!\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def create_monthly_means(input_file, output_file, variables_to_average=None):\n",
    "    \"\"\"\n",
    "    Create monthly mean averages from daily NetCDF data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_file : str\n",
    "        Path to input NetCDF file with daily data\n",
    "    output_file : str\n",
    "        Path for output NetCDF file with monthly means\n",
    "    variables_to_average : list, optional\n",
    "        List of variables to include. If None, includes all data variables\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Loading daily dataset: {input_file}\")\n",
    "    ds = xr.open_dataset(input_file)\n",
    "    \n",
    "    print(f\"Input dataset info:\")\n",
    "    print(f\"  Time range: {ds.time.min().values} to {ds.time.max().values}\")\n",
    "    print(f\"  Time dimension: {ds.dims['time']} daily timesteps\")\n",
    "    print(f\"  Spatial dimensions: {ds.dims['latitude']} lat Ã— {ds.dims['longitude']} lon\")\n",
    "    print(f\"  Total data variables: {len(ds.data_vars)}\")\n",
    "    \n",
    "    # Show data variables\n",
    "    print(f\"\\nAvailable variables:\")\n",
    "    for i, var in enumerate(ds.data_vars, 1):\n",
    "        print(f\"  {i:2d}. {var}\")\n",
    "    \n",
    "    # Determine which variables to process\n",
    "    if variables_to_average is None:\n",
    "        # Default: include key SSS-related variables, exclude utility variables\n",
    "        default_vars = [\n",
    "            'smap_sss', 'smap_sss_uncertainty', 'anc_sss', 'anc_sst',\n",
    "            'smap_spd', 'smap_high_spd', 'weight', 'ice_fraction', 'land_fraction'\n",
    "        ]\n",
    "        variables_to_average = [var for var in default_vars if var in ds.data_vars]\n",
    "        \n",
    "        # If merged dataset has combined variables, include those too\n",
    "        merged_vars = [var for var in ds.data_vars if any(x in var.lower() for x in ['combined', 'l3', 'l4'])]\n",
    "        variables_to_average.extend(merged_vars)\n",
    "        \n",
    "        # Remove duplicates\n",
    "        variables_to_average = list(set(variables_to_average))\n",
    "    \n",
    "    print(f\"\\nVariables to be averaged ({len(variables_to_average)}):\")\n",
    "    for var in variables_to_average:\n",
    "        if var in ds.data_vars:\n",
    "            coverage = (~ds[var].isnull()).sum().values / ds[var].size * 100\n",
    "            print(f\"  â€¢ {var} (data coverage: {coverage:.1f}%)\")\n",
    "    \n",
    "    # Create monthly grouping\n",
    "    print(f\"\\nCreating monthly averages...\")\n",
    "    print(\"This may take several minutes for large datasets...\")\n",
    "    \n",
    "    # Group by year-month and calculate means\n",
    "    monthly_means = ds.groupby('time.month').mean('time', skipna=True)\n",
    "    \n",
    "    # Also create year-month grouping for more detailed monthly averages\n",
    "    print(\"Creating detailed year-month averages...\")\n",
    "    \n",
    "    # Add year-month coordinate for grouping\n",
    "    ds_with_yearmonth = ds.assign_coords(year_month=ds.time.dt.strftime('%Y-%m'))\n",
    "    monthly_detailed = ds_with_yearmonth.groupby('year_month').mean('time', skipna=True)\n",
    "    \n",
    "    # Create a proper time coordinate for the monthly data\n",
    "    # Use the 1st of each month as representative date\n",
    "    year_months = pd.to_datetime(monthly_detailed.year_month.values, format='%Y-%m')\n",
    "    monthly_time_coord = year_months  # 1st of each month (default for pd.to_datetime)\n",
    "    \n",
    "    # Replace the year_month coordinate with proper datetime\n",
    "    monthly_detailed = monthly_detailed.drop_vars('year_month')\n",
    "    monthly_detailed = monthly_detailed.rename({'year_month': 'time'})\n",
    "    monthly_detailed = monthly_detailed.assign_coords(time=monthly_time_coord)\n",
    "    \n",
    "    # Sort by time\n",
    "    monthly_detailed = monthly_detailed.sortby('time')\n",
    "    \n",
    "    print(f\"âœ… Monthly averaging complete!\")\n",
    "    print(f\"  Original daily data: {ds.dims['time']} timesteps\")\n",
    "    print(f\"  Monthly averages: {monthly_detailed.dims['time']} months\")\n",
    "    print(f\"  Time range: {monthly_detailed.time.min().values} to {monthly_detailed.time.max().values}\")\n",
    "    \n",
    "    # Calculate statistics for each variable\n",
    "    print(f\"\\nMonthly averaging statistics:\")\n",
    "    for var in variables_to_average:\n",
    "        if var in ds.data_vars:\n",
    "            daily_valid = (~ds[var].isnull()).sum().values\n",
    "            monthly_valid = (~monthly_detailed[var].isnull()).sum().values\n",
    "            \n",
    "            print(f\"  {var}:\")\n",
    "            print(f\"    Daily valid points: {daily_valid:,}\")\n",
    "            print(f\"    Monthly valid points: {monthly_valid:,}\")\n",
    "            print(f\"    Data retention: {monthly_valid/daily_valid*100:.1f}%\")\n",
    "    \n",
    "    # Add metadata\n",
    "    monthly_detailed.attrs = ds.attrs.copy()\n",
    "    monthly_detailed.attrs['description'] = 'Monthly mean averages of daily SMAP SSS data'\n",
    "    monthly_detailed.attrs['temporal_resolution'] = 'monthly'\n",
    "    monthly_detailed.attrs['temporal_averaging'] = 'monthly_mean'\n",
    "    monthly_detailed.attrs['averaging_method'] = 'arithmetic_mean_skipna'\n",
    "    monthly_detailed.attrs['source_temporal_resolution'] = 'daily'\n",
    "    monthly_detailed.attrs['monthly_averaging_date'] = pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    monthly_detailed.attrs['original_time_range'] = f\"{ds.time.min().values} to {ds.time.max().values}\"\n",
    "    monthly_detailed.attrs['monthly_time_range'] = f\"{monthly_detailed.time.min().values} to {monthly_detailed.time.max().values}\"\n",
    "    monthly_detailed.attrs['original_timesteps'] = ds.dims['time']\n",
    "    monthly_detailed.attrs['monthly_timesteps'] = monthly_detailed.dims['time']\n",
    "    \n",
    "    # Add variable-specific metadata\n",
    "    for var in monthly_detailed.data_vars:\n",
    "        if var in ds.data_vars:\n",
    "            monthly_detailed[var].attrs = ds[var].attrs.copy()\n",
    "            monthly_detailed[var].attrs['temporal_averaging'] = 'monthly_mean'\n",
    "            monthly_detailed[var].attrs['averaging_period'] = 'calendar_month'\n",
    "    \n",
    "    # Save monthly dataset\n",
    "    print(f\"\\nSaving monthly averages to {output_file}...\")\n",
    "    \n",
    "    # Use compression for efficiency\n",
    "    encoding = {}\n",
    "    for var in monthly_detailed.data_vars:\n",
    "        if monthly_detailed[var].dtype == 'float32':\n",
    "            encoding[var] = {\n",
    "                'zlib': True, \n",
    "                'complevel': 4,\n",
    "                'shuffle': True,\n",
    "                '_FillValue': np.nan\n",
    "            }\n",
    "        else:\n",
    "            encoding[var] = {'zlib': True, 'complevel': 4}\n",
    "    \n",
    "    monthly_detailed.to_netcdf(output_file, encoding=encoding)\n",
    "    \n",
    "    print(f\"âœ… Monthly averages saved: {output_file}\")\n",
    "    \n",
    "    # Quick verification\n",
    "    print(f\"\\n=== VERIFICATION ===\")\n",
    "    monthly_check = xr.open_dataset(output_file)\n",
    "    print(f\"Monthly dataset shape: {monthly_check.dims}\")\n",
    "    print(f\"File size reduction: {ds.nbytes / monthly_check.nbytes:.1f}x smaller\")\n",
    "    \n",
    "    # Show sample of time coordinate\n",
    "    print(f\"\\nSample monthly time coordinates:\")\n",
    "    for i in range(min(10, len(monthly_check.time))):\n",
    "        date_str = pd.to_datetime(monthly_check.time[i].values).strftime('%Y-%m-%d')\n",
    "        print(f\"  Month {i+1}: {date_str}\")\n",
    "    \n",
    "    if len(monthly_check.time) > 10:\n",
    "        print(f\"  ... and {len(monthly_check.time)-10} more months\")\n",
    "    \n",
    "    monthly_check.close()\n",
    "    ds.close()\n",
    "    \n",
    "    return monthly_detailed\n",
    "\n",
    "def create_seasonal_means(monthly_file, output_file):\n",
    "    \"\"\"\n",
    "    Create seasonal averages from monthly data\n",
    "    \"\"\"\n",
    "    print(f\"\\nCreating seasonal averages from {monthly_file}...\")\n",
    "    \n",
    "    monthly_ds = xr.open_dataset(monthly_file)\n",
    "    \n",
    "    # Define seasons\n",
    "    season_groups = monthly_ds.groupby('time.season').mean('time', skipna=True)\n",
    "    \n",
    "    # Create seasonal time coordinates (use middle of season)\n",
    "    season_dates = {\n",
    "        'DJF': pd.Timestamp('2020-01-15'),  # Winter\n",
    "        'MAM': pd.Timestamp('2020-04-15'),  # Spring  \n",
    "        'JJA': pd.Timestamp('2020-07-15'),  # Summer\n",
    "        'SON': pd.Timestamp('2020-10-15'),  # Fall\n",
    "    }\n",
    "    \n",
    "    season_coords = [season_dates[season] for season in season_groups.season.values]\n",
    "    season_groups = season_groups.assign_coords(time=('season', season_coords))\n",
    "    season_groups = season_groups.swap_dims({'season': 'time'})\n",
    "    \n",
    "    # Add metadata\n",
    "    season_groups.attrs = monthly_ds.attrs.copy()\n",
    "    season_groups.attrs['temporal_resolution'] = 'seasonal'\n",
    "    season_groups.attrs['temporal_averaging'] = 'seasonal_mean'\n",
    "    season_groups.attrs['seasons'] = 'DJF=Winter, MAM=Spring, JJA=Summer, SON=Fall'\n",
    "    \n",
    "    # Save seasonal dataset\n",
    "    encoding = {var: {'zlib': True, 'complevel': 4} for var in season_groups.data_vars}\n",
    "    season_groups.to_netcdf(output_file, encoding=encoding)\n",
    "    \n",
    "    print(f\"Seasonal averages saved: {output_file}\")\n",
    "    monthly_ds.close()\n",
    "    \n",
    "    return season_groups\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # File paths\n",
    "    input_file = \"SMAP_Daily_subset_combined_OISSSfilled.nc4\"\n",
    "    monthly_output = \"SMAP_MO_subset_combined.nc4\"\n",
    "    seasonal_output = \"SMAP_Seasonal_subset_combined.nc4\"\n",
    "    \n",
    "    print(\"=== CREATING MONTHLY AVERAGES ===\")\n",
    "    monthly_data = create_monthly_means(input_file, monthly_output)\n",
    "    \n",
    "    print(\"\\n=== CREATING SEASONAL AVERAGES ===\")\n",
    "    seasonal_data = create_seasonal_means(monthly_output, seasonal_output)\n",
    "    \n",
    "    print(f\"   1. {monthly_output} (Monthly averages)\")\n",
    "    print(f\"   2. {seasonal_output} (Seasonal averages)\")\n",
    "    print(f\"\\nYou can now use these averaged datasets for analysis!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23761147-44e8-4c52-a175-25ce3bddd0a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
