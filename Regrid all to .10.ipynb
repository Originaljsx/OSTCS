{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2221c01e-5153-4532-8f55-ac22c89ee47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting regridding process...\n",
      "Target resolution: 0.1°\n",
      "\n",
      "==================================================\n",
      "Processing CHLOR_A\n",
      "==================================================\n",
      "Loading dataset...\n",
      "Successfully loaded: study_data/CHL_MO_subset_combined.nc4\n",
      "Dataset dimensions: {'time': 121, 'lat': 2040, 'lon': 3120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Data variables: ['chlor_a', 'palette']\n",
      "Current resolution: 0.041672°\n",
      "Target resolution: 0.1°\n",
      "→ Upscaling (coarsening) required\n",
      "  Coarsening with weight: 3\n",
      "  From 0.0417° to ~0.1250°\n",
      "\n",
      "--- Validation for chlor_a ---\n",
      "Original resolution: 0.041672°\n",
      "New resolution: 0.125000°\n",
      "Target resolution: 0.1°\n",
      "Resolution error: 0.025000°\n",
      "Original shape: {'time': 121, 'lat': 2040, 'lon': 3120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "New shape: {'time': 121, 'lat': 680, 'lon': 1040, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Longitude range: [-179.938, -50.062]\n",
      "Latitude range: [-4.938, 79.938]\n",
      "Time dimension: 121 → 121\n",
      "Time range: 2015-05-01T00:00:00.000000000 to 2025-05-01T00:00:00.000000000\n",
      "\n",
      "==================================================\n",
      "Processing SST\n",
      "==================================================\n",
      "Loading dataset...\n",
      "Successfully loaded: study_data/OSTIA_MO_subset_combined.nc\n",
      "Dataset dimensions: {'time': 121, 'lat': 1704, 'lon': 2602}\n",
      "Data variables: ['analysed_sst', 'analysis_error', 'sea_ice_fraction', 'mask']\n",
      "Current resolution: 0.050003°\n",
      "Target resolution: 0.1°\n",
      "→ Upscaling (coarsening) required\n",
      "  Coarsening with weight: 2\n",
      "  From 0.0500° to ~0.1000°\n",
      "\n",
      "--- Validation for sst ---\n",
      "Original resolution: 0.050003°\n",
      "New resolution: 0.100006°\n",
      "Target resolution: 0.1°\n",
      "Resolution error: 0.000006°\n",
      "Original shape: {'time': 121, 'lat': 1704, 'lon': 2602}\n",
      "New shape: {'time': 121, 'lat': 852, 'lon': 1301}\n",
      "Longitude range: [-179.950, -49.950]\n",
      "Latitude range: [-5.050, 80.050]\n",
      "Time dimension: 121 → 121\n",
      "Time range: 2015-05-01T00:00:00.000000000 to 2025-05-01T00:00:00.000000000\n",
      "\n",
      "==================================================\n",
      "Processing SSS\n",
      "==================================================\n",
      "Loading dataset...\n",
      "Successfully loaded: study_data/SMAP_MO_subset_combined_OISSSfilled.nc4\n",
      "Dataset dimensions: {'time': 121, 'latitude': 340, 'longitude': 520}\n",
      "Data variables: ['smap_sss_uncertainty', 'smap_spd', 'ice_fraction', 'anc_sst', 'weight', 'land_fraction', 'smap_high_spd', 'smap_sss', 'anc_sss', 'spatial_ref']\n",
      "Current resolution: 0.250000°\n",
      "Target resolution: 0.1°\n",
      "→ Downscaling (interpolation) required\n",
      "  Interpolating with factor: 2\n",
      "  From 0.2500° to 0.1000°\n",
      "\n",
      "--- Validation for sss ---\n",
      "Original resolution: 0.250000°\n",
      "New resolution: 0.124880°\n",
      "Target resolution: 0.1°\n",
      "Resolution error: 0.024880°\n",
      "Original shape: {'time': 121, 'latitude': 340, 'longitude': 520}\n",
      "New shape: {'time': 121, 'latitude': 680, 'longitude': 1040}\n",
      "Longitude range: [-179.875, -50.125]\n",
      "Latitude range: [-4.875, 79.875]\n",
      "Time dimension: 121 → 121\n",
      "Time range: 2015-05-01T00:00:00.000000000 to 2025-05-01T00:00:00.000000000\n",
      "\n",
      "==================================================\n",
      "SAVING REGRIDDED DATASETS\n",
      "==================================================\n",
      "Saving chlor_a to chlor_a_010deg.nc4...\n",
      "✓ Successfully saved chlor_a_010deg.nc4\n",
      "  File size: ~326.5 MB\n",
      "Saving sst to sst_010deg.nc4...\n",
      "✓ Successfully saved sst_010deg.nc4\n",
      "  File size: ~2046.6 MB\n",
      "Saving sss to sss_010deg.nc4...\n",
      "✓ Successfully saved sss_010deg.nc4\n",
      "  File size: ~5875.7 MB\n",
      "\n",
      "==================================================\n",
      "REGRIDDING COMPLETE!\n",
      "==================================================\n",
      "All datasets have been regridded to 0.10° resolution.\n",
      "You can now use these files for comparative analysis.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Regrid multiple ocean datasets to a common 0.10° spatial resolution.\n",
    "\n",
    "This script processes:\n",
    "- AQUA/MODIS Chlor_a (0.04° → 0.10°) - upscaling using coarsen\n",
    "- OSTIA SST (0.05° → 0.10°) - upscaling using coarsen  \n",
    "- SMAP SSS (0.25° → 0.10°) - downscaling using interpolation\n",
    "\n",
    "All datasets should be monthly data from 2015-05 to 2025-05 on the same spatial bounds.\n",
    "\"\"\"\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configuration\n",
    "TARGET_RESOLUTION = 0.10  # degrees\n",
    "\n",
    "# File paths\n",
    "file_paths = {\n",
    "    'chlor_a': 'study_data/CHL_MO_subset_combined.nc4',           # 0.04° resolution\n",
    "    'sst': 'study_data/OSTIA_MO_subset_combined.nc',             # 0.05° resolution\n",
    "    'sss': 'study_data/SMAP_MO_subset_combined_OISSSfilled.nc4'  # 0.25° resolution\n",
    "}\n",
    "\n",
    "# Output file paths\n",
    "output_paths = {\n",
    "    'chlor_a': 'chlor_a_010deg.nc4',\n",
    "    'sst': 'sst_010deg.nc4', \n",
    "    'sss': 'sss_010deg.nc4'\n",
    "}\n",
    "\n",
    "def get_grid_resolution(dataset):\n",
    "    \"\"\"Calculate the grid resolution of a dataset.\"\"\"\n",
    "    # Handle different longitude coordinate names\n",
    "    if 'longitude' in dataset.coords:\n",
    "        lon_coord = 'longitude'\n",
    "    elif 'lon' in dataset.coords:\n",
    "        lon_coord = 'lon'\n",
    "    else:\n",
    "        raise ValueError(\"No longitude coordinate found. Expected 'longitude' or 'lon'\")\n",
    "    \n",
    "    coords = dataset[lon_coord].values\n",
    "    return float(coords[1] - coords[0])\n",
    "\n",
    "def coarsen_dataset(dataset, input_res, target_res, data_vars=None):\n",
    "    \"\"\"\n",
    "    Coarsen (reduce resolution) of a dataset using xarray.coarsen().\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset : xarray.Dataset\n",
    "        Input dataset to coarsen\n",
    "    input_res : float\n",
    "        Input grid resolution in degrees\n",
    "    target_res : float  \n",
    "        Target grid resolution in degrees\n",
    "    data_vars : list, optional\n",
    "        List of data variables to process. If None, processes all data variables.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    xarray.Dataset\n",
    "        Coarsened dataset\n",
    "    \"\"\"\n",
    "    # Calculate coarsening weight\n",
    "    weight = int(np.ceil(target_res / input_res))\n",
    "    \n",
    "    print(f\"  Coarsening with weight: {weight}\")\n",
    "    print(f\"  From {input_res:.4f}° to ~{input_res * weight:.4f}°\")\n",
    "    \n",
    "    # Handle different coordinate names\n",
    "    lon_coord = 'longitude' if 'longitude' in dataset.coords else 'lon'\n",
    "    lat_coord = 'latitude' if 'latitude' in dataset.coords else 'lat'\n",
    "    \n",
    "    # Apply coarsening to longitude and latitude dimensions\n",
    "    coarsened = dataset.coarsen(**{lon_coord: weight}, boundary=\"pad\").mean() \\\n",
    "                      .coarsen(**{lat_coord: weight}, boundary=\"pad\").mean()\n",
    "    \n",
    "    return coarsened\n",
    "\n",
    "def interpolate_dataset(dataset, input_res, target_res, data_vars=None):\n",
    "    \"\"\"\n",
    "    Interpolate (increase resolution) of a dataset using xarray.interp().\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset : xarray.Dataset\n",
    "        Input dataset to interpolate\n",
    "    input_res : float\n",
    "        Input grid resolution in degrees\n",
    "    target_res : float\n",
    "        Target grid resolution in degrees\n",
    "    data_vars : list, optional\n",
    "        List of data variables to process. If None, processes all data variables.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    xarray.Dataset\n",
    "        Interpolated dataset\n",
    "    \"\"\"\n",
    "    # Calculate increase factor\n",
    "    increase_factor = int(input_res / target_res)\n",
    "    \n",
    "    print(f\"  Interpolating with factor: {increase_factor}\")\n",
    "    print(f\"  From {input_res:.4f}° to {target_res:.4f}°\")\n",
    "    \n",
    "    # Handle different coordinate names\n",
    "    lon_coord = 'longitude' if 'longitude' in dataset.coords else 'lon'\n",
    "    lat_coord = 'latitude' if 'latitude' in dataset.coords else 'lat'\n",
    "    \n",
    "    # Create new coordinate arrays\n",
    "    new_lon = np.linspace(\n",
    "        dataset[lon_coord][0], \n",
    "        dataset[lon_coord][-1], \n",
    "        dataset.dims[lon_coord] * increase_factor\n",
    "    )\n",
    "    new_lat = np.linspace(\n",
    "        dataset[lat_coord][0], \n",
    "        dataset[lat_coord][-1], \n",
    "        dataset.dims[lat_coord] * increase_factor\n",
    "    )\n",
    "    \n",
    "    # Perform interpolation using proper coordinate names\n",
    "    interp_coords = {lon_coord: new_lon, lat_coord: new_lat}\n",
    "    interpolated = dataset.interp(**interp_coords)\n",
    "    \n",
    "    return interpolated\n",
    "\n",
    "def validate_regridding(original, regridded, dataset_name):\n",
    "    \"\"\"Validate the regridding results.\"\"\"\n",
    "    print(f\"\\n--- Validation for {dataset_name} ---\")\n",
    "    \n",
    "    # Handle different coordinate names\n",
    "    lon_coord = 'longitude' if 'longitude' in regridded.coords else 'lon'\n",
    "    lat_coord = 'latitude' if 'latitude' in regridded.coords else 'lat'\n",
    "    \n",
    "    # Check resolution\n",
    "    original_res = get_grid_resolution(original)\n",
    "    new_res = get_grid_resolution(regridded)\n",
    "    \n",
    "    print(f\"Original resolution: {original_res:.6f}°\")\n",
    "    print(f\"New resolution: {new_res:.6f}°\")\n",
    "    print(f\"Target resolution: {TARGET_RESOLUTION}°\")\n",
    "    print(f\"Resolution error: {abs(new_res - TARGET_RESOLUTION):.6f}°\")\n",
    "    \n",
    "    # Check dimensions\n",
    "    print(f\"Original shape: {dict(original.dims)}\")\n",
    "    print(f\"New shape: {dict(regridded.dims)}\")\n",
    "    \n",
    "    # Check coordinate ranges\n",
    "    print(f\"Longitude range: [{regridded[lon_coord].min().values:.3f}, {regridded[lon_coord].max().values:.3f}]\")\n",
    "    print(f\"Latitude range: [{regridded[lat_coord].min().values:.3f}, {regridded[lat_coord].max().values:.3f}]\")\n",
    "    \n",
    "    # Check time dimension (should be unchanged)\n",
    "    if 'time' in original.dims and 'time' in regridded.dims:\n",
    "        print(f\"Time dimension: {original.dims['time']} → {regridded.dims['time']}\")\n",
    "        print(f\"Time range: {regridded.time.min().values} to {regridded.time.max().values}\")\n",
    "\n",
    "def process_dataset(file_path, dataset_name, target_resolution):\n",
    "    \"\"\"Process a single dataset file.\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing {dataset_name.upper()}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Load dataset\n",
    "    print(\"Loading dataset...\")\n",
    "    try:\n",
    "        ds = xr.open_dataset(file_path)\n",
    "        print(f\"Successfully loaded: {file_path}\")\n",
    "        print(f\"Dataset dimensions: {dict(ds.dims)}\")\n",
    "        print(f\"Data variables: {list(ds.data_vars)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Get current resolution\n",
    "    current_res = get_grid_resolution(ds)\n",
    "    print(f\"Current resolution: {current_res:.6f}°\")\n",
    "    print(f\"Target resolution: {target_resolution}°\")\n",
    "    \n",
    "    # Determine if we need to coarsen or interpolate\n",
    "    if current_res < target_resolution:\n",
    "        print(\"→ Upscaling (coarsening) required\")\n",
    "        regridded = coarsen_dataset(ds, current_res, target_resolution)\n",
    "    elif current_res > target_resolution:\n",
    "        print(\"→ Downscaling (interpolation) required\")\n",
    "        regridded = interpolate_dataset(ds, current_res, target_resolution)\n",
    "    else:\n",
    "        print(\"→ No regridding needed, resolution already matches target\")\n",
    "        regridded = ds\n",
    "    \n",
    "    # Validate results\n",
    "    validate_regridding(ds, regridded, dataset_name)\n",
    "    \n",
    "    # Close original dataset\n",
    "    ds.close()\n",
    "    \n",
    "    return regridded\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main processing function.\"\"\"\n",
    "    print(\"Starting regridding process...\")\n",
    "    print(f\"Target resolution: {TARGET_RESOLUTION}°\")\n",
    "    \n",
    "    # Process each dataset\n",
    "    regridded_datasets = {}\n",
    "    \n",
    "    for dataset_name, file_path in file_paths.items():\n",
    "        regridded = process_dataset(file_path, dataset_name, TARGET_RESOLUTION)\n",
    "        if regridded is not None:\n",
    "            regridded_datasets[dataset_name] = regridded\n",
    "    \n",
    "    # Save regridded datasets\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"SAVING REGRIDDED DATASETS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    for dataset_name, regridded in regridded_datasets.items():\n",
    "        output_file = output_paths[dataset_name]\n",
    "        print(f\"Saving {dataset_name} to {output_file}...\")\n",
    "        \n",
    "        try:\n",
    "            # Add attributes to document the regridding\n",
    "            regridded.attrs['regridding_info'] = f'Regridded to {TARGET_RESOLUTION}° resolution'\n",
    "            regridded.attrs['regridding_method'] = 'xarray coarsen/interp'\n",
    "            regridded.attrs['processing_date'] = str(np.datetime64('now'))\n",
    "            \n",
    "            # Save with compression\n",
    "            encoding = {}\n",
    "            for var in regridded.data_vars:\n",
    "                encoding[var] = {'zlib': True, 'complevel': 4}\n",
    "            \n",
    "            regridded.to_netcdf(output_file, encoding=encoding)\n",
    "            print(f\"✓ Successfully saved {output_file}\")\n",
    "            \n",
    "            # Print file info\n",
    "            file_size = regridded.nbytes / (1024**2)  # MB\n",
    "            print(f\"  File size: ~{file_size:.1f} MB\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error saving {output_file}: {e}\")\n",
    "        \n",
    "        # Close dataset\n",
    "        regridded.close()\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"REGRIDDING COMPLETE!\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(\"All datasets have been regridded to 0.10° resolution.\")\n",
    "    print(\"You can now use these files for comparative analysis.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example of how to customize for specific variable names\n",
    "    # Uncomment and modify if your datasets have different variable names\n",
    "    \n",
    "    # For MODIS Chlor_a - common variable names: 'chlor_a', 'CHL', 'chl'\n",
    "    # For OSTIA SST - common variable names: 'analysed_sst', 'sst', 'SST'  \n",
    "    # For SMAP SSS - common variable names: 'sss', 'SSS', 'salinity'\n",
    "    \n",
    "    main()\n",
    "\n",
    "    # Optional: Quick validation plot (uncomment if you want visual validation)\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Load one of the regridded files for quick visual check\n",
    "    test_file = output_paths['sst']  # or whichever you want to check\n",
    "    ds_test = xr.open_dataset(test_file)\n",
    "    \n",
    "    # Plot first time step\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    ds_test.isel(time=0).plot()\n",
    "    plt.title(f'Regridded data - First time step')\n",
    "    plt.show()\n",
    "    \n",
    "    ds_test.close()\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4741578d-1bb1-4527-b0b4-67ec7b162a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting regridding process...\n",
      "Target resolution: 0.1°\n",
      "\n",
      "==================================================\n",
      "Processing CHLOR_A\n",
      "==================================================\n",
      "Loading dataset...\n",
      "Successfully loaded: study_data/CHL_MO_subset_combined.nc4\n",
      "Dataset dimensions: {'time': 121, 'lat': 2040, 'lon': 3120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Data variables: ['chlor_a', 'palette']\n",
      "Current resolution: 0.041672°\n",
      "Target resolution: 0.1°\n",
      "→ Step 1: Upscaling (coarsening) required\n",
      "  Coarsening with weight: 3\n",
      "  From 0.0417° to ~0.1250°\n",
      "→ Step 2: Final interpolation to exact 0.1°\n",
      "  Intermediate resolution: 0.125000°\n",
      "  Final interpolation - Longitude resolution: 0.099981°\n",
      "  Final interpolation - Latitude resolution: 0.099971°\n",
      "\n",
      "--- Validation for chlor_a ---\n",
      "Original resolution: 0.041672°\n",
      "New resolution: 0.099981°\n",
      "Target resolution: 0.1°\n",
      "Resolution error: 0.000019°\n",
      "Original shape: {'time': 121, 'lat': 2040, 'lon': 3120, 'rgb': 3, 'eightbitcolor': 256}\n",
      "New shape: {'time': 121, 'lat': 850, 'lon': 1300, 'rgb': 3, 'eightbitcolor': 256}\n",
      "Longitude range: [-179.938, -50.062]\n",
      "Latitude range: [-4.938, 79.938]\n",
      "Time dimension: 121 → 121\n",
      "Time range: 2015-05-01T00:00:00.000000000 to 2025-05-01T00:00:00.000000000\n",
      "\n",
      "==================================================\n",
      "Processing SST\n",
      "==================================================\n",
      "Loading dataset...\n",
      "Successfully loaded: study_data/OSTIA_MO_subset_combined.nc\n",
      "Dataset dimensions: {'time': 121, 'lat': 1704, 'lon': 2602}\n",
      "Data variables: ['analysed_sst', 'analysis_error', 'sea_ice_fraction', 'mask']\n",
      "Current resolution: 0.050003°\n",
      "Target resolution: 0.1°\n",
      "→ Step 1: Upscaling (coarsening) required\n",
      "  Coarsening with weight: 2\n",
      "  From 0.0500° to ~0.1000°\n",
      "→ Step 2: Final interpolation to exact 0.1°\n",
      "  Intermediate resolution: 0.100006°\n",
      "  Final interpolation - Longitude resolution: 0.100000°\n",
      "  Final interpolation - Latitude resolution: 0.100000°\n",
      "\n",
      "--- Validation for sst ---\n",
      "Original resolution: 0.050003°\n",
      "New resolution: 0.100000°\n",
      "Target resolution: 0.1°\n",
      "Resolution error: 0.000000°\n",
      "Original shape: {'time': 121, 'lat': 1704, 'lon': 2602}\n",
      "New shape: {'time': 121, 'lat': 852, 'lon': 1301}\n",
      "Longitude range: [-179.950, -49.950]\n",
      "Latitude range: [-5.050, 80.050]\n",
      "Time dimension: 121 → 121\n",
      "Time range: 2015-05-01T00:00:00.000000000 to 2025-05-01T00:00:00.000000000\n",
      "\n",
      "==================================================\n",
      "Processing SSS\n",
      "==================================================\n",
      "Loading dataset...\n",
      "Successfully loaded: study_data/SMAP_MO_subset_combined_OISSSfilled.nc4\n",
      "Dataset dimensions: {'time': 121, 'latitude': 340, 'longitude': 520}\n",
      "Data variables: ['smap_sss_uncertainty', 'smap_spd', 'ice_fraction', 'anc_sst', 'weight', 'land_fraction', 'smap_high_spd', 'smap_sss', 'anc_sss', 'spatial_ref']\n",
      "Current resolution: 0.250000°\n",
      "Target resolution: 0.1°\n",
      "→ Step 1: Downscaling (interpolation) required\n",
      "  Interpolating with factor: 2\n",
      "  From 0.2500° to 0.1000°\n",
      "→ Step 2: Final interpolation to exact 0.1°\n",
      "  Intermediate resolution: 0.124880°\n",
      "  Final interpolation - Longitude resolution: 0.099961°\n",
      "  Final interpolation - Latitude resolution: 0.099941°\n",
      "\n",
      "--- Validation for sss ---\n",
      "Original resolution: 0.250000°\n",
      "New resolution: 0.099961°\n",
      "Target resolution: 0.1°\n",
      "Resolution error: 0.000039°\n",
      "Original shape: {'time': 121, 'latitude': 340, 'longitude': 520}\n",
      "New shape: {'time': 121, 'latitude': 849, 'longitude': 1299}\n",
      "Longitude range: [-179.875, -50.125]\n",
      "Latitude range: [-4.875, 79.875]\n",
      "Time dimension: 121 → 121\n",
      "Time range: 2015-05-01T00:00:00.000000000 to 2025-05-01T00:00:00.000000000\n",
      "\n",
      "==================================================\n",
      "SAVING REGRIDDED DATASETS\n",
      "==================================================\n",
      "Saving chlor_a to chlor_a_010deg.nc4...\n",
      "✓ Successfully saved chlor_a_010deg.nc4\n",
      "  File size: ~1020.2 MB\n",
      "Saving sst to sst_010deg.nc4...\n",
      "✓ Successfully saved sst_010deg.nc4\n",
      "  File size: ~4093.1 MB\n",
      "Saving sss to sss_010deg.nc4...\n",
      "✓ Successfully saved sss_010deg.nc4\n",
      "  File size: ~9163.0 MB\n",
      "\n",
      "==================================================\n",
      "REGRIDDING COMPLETE!\n",
      "==================================================\n",
      "All datasets have been regridded to 0.10° resolution.\n",
      "You can now use these files for comparative analysis.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Regrid multiple ocean datasets to a common 0.10° spatial resolution.\n",
    "\n",
    "This script processes:\n",
    "- AQUA/MODIS Chlor_a (0.04° → 0.10°) - upscaling using coarsen\n",
    "- OSTIA SST (0.05° → 0.10°) - upscaling using coarsen  \n",
    "- SMAP SSS (0.25° → 0.10°) - downscaling using interpolation\n",
    "\n",
    "All datasets should be monthly data from 2015-05 to 2025-05 on the same spatial bounds.\n",
    "\"\"\"\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configuration\n",
    "TARGET_RESOLUTION = 0.10  # degrees\n",
    "\n",
    "# File paths\n",
    "file_paths = {\n",
    "    'chlor_a': 'study_data/CHL_MO_subset_combined.nc4',           # 0.04° resolution\n",
    "    'sst': 'study_data/OSTIA_MO_subset_combined.nc',             # 0.05° resolution\n",
    "    'sss': 'study_data/SMAP_MO_subset_combined_OISSSfilled.nc4'  # 0.25° resolution\n",
    "}\n",
    "\n",
    "# Output file paths\n",
    "output_paths = {\n",
    "    'chlor_a': 'chlor_a_010deg.nc4',\n",
    "    'sst': 'sst_010deg.nc4', \n",
    "    'sss': 'sss_010deg.nc4'\n",
    "}\n",
    "\n",
    "def get_grid_resolution(dataset):\n",
    "    \"\"\"Calculate the grid resolution of a dataset.\"\"\"\n",
    "    # Handle different longitude coordinate names\n",
    "    if 'longitude' in dataset.coords:\n",
    "        lon_coord = 'longitude'\n",
    "    elif 'lon' in dataset.coords:\n",
    "        lon_coord = 'lon'\n",
    "    else:\n",
    "        raise ValueError(\"No longitude coordinate found. Expected 'longitude' or 'lon'\")\n",
    "    \n",
    "    coords = dataset[lon_coord].values\n",
    "    return float(coords[1] - coords[0])\n",
    "\n",
    "def coarsen_dataset(dataset, input_res, target_res, data_vars=None):\n",
    "    \"\"\"\n",
    "    Coarsen (reduce resolution) of a dataset using xarray.coarsen().\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset : xarray.Dataset\n",
    "        Input dataset to coarsen\n",
    "    input_res : float\n",
    "        Input grid resolution in degrees\n",
    "    target_res : float  \n",
    "        Target grid resolution in degrees\n",
    "    data_vars : list, optional\n",
    "        List of data variables to process. If None, processes all data variables.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    xarray.Dataset\n",
    "        Coarsened dataset\n",
    "    \"\"\"\n",
    "    # Calculate coarsening weight\n",
    "    weight = int(np.ceil(target_res / input_res))\n",
    "    \n",
    "    print(f\"  Coarsening with weight: {weight}\")\n",
    "    print(f\"  From {input_res:.4f}° to ~{input_res * weight:.4f}°\")\n",
    "    \n",
    "    # Handle different coordinate names\n",
    "    lon_coord = 'longitude' if 'longitude' in dataset.coords else 'lon'\n",
    "    lat_coord = 'latitude' if 'latitude' in dataset.coords else 'lat'\n",
    "    \n",
    "    # Apply coarsening to longitude and latitude dimensions\n",
    "    coarsened = dataset.coarsen(**{lon_coord: weight}, boundary=\"pad\").mean() \\\n",
    "                      .coarsen(**{lat_coord: weight}, boundary=\"pad\").mean()\n",
    "    \n",
    "    return coarsened\n",
    "\n",
    "def interpolate_dataset(dataset, input_res, target_res, data_vars=None):\n",
    "    \"\"\"\n",
    "    Interpolate (increase resolution) of a dataset using xarray.interp().\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset : xarray.Dataset\n",
    "        Input dataset to interpolate\n",
    "    input_res : float\n",
    "        Input grid resolution in degrees\n",
    "    target_res : float\n",
    "        Target grid resolution in degrees\n",
    "    data_vars : list, optional\n",
    "        List of data variables to process. If None, processes all data variables.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    xarray.Dataset\n",
    "        Interpolated dataset\n",
    "    \"\"\"\n",
    "    # Calculate increase factor\n",
    "    increase_factor = int(input_res / target_res)\n",
    "    \n",
    "    print(f\"  Interpolating with factor: {increase_factor}\")\n",
    "    print(f\"  From {input_res:.4f}° to {target_res:.4f}°\")\n",
    "    \n",
    "    # Handle different coordinate names\n",
    "    lon_coord = 'longitude' if 'longitude' in dataset.coords else 'lon'\n",
    "    lat_coord = 'latitude' if 'latitude' in dataset.coords else 'lat'\n",
    "    \n",
    "    # Create new coordinate arrays\n",
    "    new_lon = np.linspace(\n",
    "        dataset[lon_coord][0], \n",
    "        dataset[lon_coord][-1], \n",
    "        dataset.dims[lon_coord] * increase_factor\n",
    "    )\n",
    "    new_lat = np.linspace(\n",
    "        dataset[lat_coord][0], \n",
    "        dataset[lat_coord][-1], \n",
    "        dataset.dims[lat_coord] * increase_factor\n",
    "    )\n",
    "    \n",
    "    # Perform interpolation using proper coordinate names\n",
    "    interp_coords = {lon_coord: new_lon, lat_coord: new_lat}\n",
    "    interpolated = dataset.interp(**interp_coords)\n",
    "    \n",
    "    return interpolated\n",
    "\n",
    "def validate_regridding(original, regridded, dataset_name):\n",
    "    \"\"\"Validate the regridding results.\"\"\"\n",
    "    print(f\"\\n--- Validation for {dataset_name} ---\")\n",
    "    \n",
    "    # Handle different coordinate names\n",
    "    lon_coord = 'longitude' if 'longitude' in regridded.coords else 'lon'\n",
    "    lat_coord = 'latitude' if 'latitude' in regridded.coords else 'lat'\n",
    "    \n",
    "    # Check resolution\n",
    "    original_res = get_grid_resolution(original)\n",
    "    new_res = get_grid_resolution(regridded)\n",
    "    \n",
    "    print(f\"Original resolution: {original_res:.6f}°\")\n",
    "    print(f\"New resolution: {new_res:.6f}°\")\n",
    "    print(f\"Target resolution: {TARGET_RESOLUTION}°\")\n",
    "    print(f\"Resolution error: {abs(new_res - TARGET_RESOLUTION):.6f}°\")\n",
    "    \n",
    "    # Check dimensions\n",
    "    print(f\"Original shape: {dict(original.dims)}\")\n",
    "    print(f\"New shape: {dict(regridded.dims)}\")\n",
    "    \n",
    "    # Check coordinate ranges\n",
    "    print(f\"Longitude range: [{regridded[lon_coord].min().values:.3f}, {regridded[lon_coord].max().values:.3f}]\")\n",
    "    print(f\"Latitude range: [{regridded[lat_coord].min().values:.3f}, {regridded[lat_coord].max().values:.3f}]\")\n",
    "    \n",
    "    # Check time dimension (should be unchanged)\n",
    "    if 'time' in original.dims and 'time' in regridded.dims:\n",
    "        print(f\"Time dimension: {original.dims['time']} → {regridded.dims['time']}\")\n",
    "        print(f\"Time range: {regridded.time.min().values} to {regridded.time.max().values}\")\n",
    "\n",
    "def final_interpolation_to_target(dataset, target_resolution):\n",
    "    \"\"\"\n",
    "    Final interpolation step to ensure exact target resolution.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset : xarray.Dataset\n",
    "        Dataset to interpolate to exact target resolution\n",
    "    target_resolution : float\n",
    "        Exact target resolution in degrees\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    xarray.Dataset\n",
    "        Dataset interpolated to exact target resolution\n",
    "    \"\"\"\n",
    "    # Handle different coordinate names\n",
    "    lon_coord = 'longitude' if 'longitude' in dataset.coords else 'lon'\n",
    "    lat_coord = 'latitude' if 'latitude' in dataset.coords else 'lat'\n",
    "    \n",
    "    # Get current coordinate ranges\n",
    "    lon_min = float(dataset[lon_coord].min())\n",
    "    lon_max = float(dataset[lon_coord].max())\n",
    "    lat_min = float(dataset[lat_coord].min())\n",
    "    lat_max = float(dataset[lat_coord].max())\n",
    "    \n",
    "    # Create new coordinate arrays with exact target resolution\n",
    "    # Calculate number of points to maintain coverage\n",
    "    n_lon = int(np.round((lon_max - lon_min) / target_resolution)) + 1\n",
    "    n_lat = int(np.round((lat_max - lat_min) / target_resolution)) + 1\n",
    "    \n",
    "    new_lon = np.linspace(lon_min, lon_max, n_lon)\n",
    "    new_lat = np.linspace(lat_min, lat_max, n_lat)\n",
    "    \n",
    "    # Verify the resolution is exactly what we want\n",
    "    actual_lon_res = new_lon[1] - new_lon[0]\n",
    "    actual_lat_res = new_lat[1] - new_lat[0]\n",
    "    \n",
    "    print(f\"  Final interpolation - Longitude resolution: {actual_lon_res:.6f}°\")\n",
    "    print(f\"  Final interpolation - Latitude resolution: {actual_lat_res:.6f}°\")\n",
    "    \n",
    "    # Perform final interpolation\n",
    "    interp_coords = {lon_coord: new_lon, lat_coord: new_lat}\n",
    "    final_dataset = dataset.interp(**interp_coords)\n",
    "    \n",
    "    return final_dataset\n",
    "\n",
    "def process_dataset(file_path, dataset_name, target_resolution):\n",
    "    \"\"\"Process a single dataset file.\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing {dataset_name.upper()}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Load dataset\n",
    "    print(\"Loading dataset...\")\n",
    "    try:\n",
    "        ds = xr.open_dataset(file_path)\n",
    "        print(f\"Successfully loaded: {file_path}\")\n",
    "        print(f\"Dataset dimensions: {dict(ds.dims)}\")\n",
    "        print(f\"Data variables: {list(ds.data_vars)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Get current resolution\n",
    "    current_res = get_grid_resolution(ds)\n",
    "    print(f\"Current resolution: {current_res:.6f}°\")\n",
    "    print(f\"Target resolution: {target_resolution}°\")\n",
    "    \n",
    "    # Step 1: Initial regridding (coarsen or interpolate to get close to target)\n",
    "    if current_res < target_resolution:\n",
    "        print(\"→ Step 1: Upscaling (coarsening) required\")\n",
    "        regridded = coarsen_dataset(ds, current_res, target_resolution)\n",
    "    elif current_res > target_resolution:\n",
    "        print(\"→ Step 1: Downscaling (interpolation) required\")\n",
    "        regridded = interpolate_dataset(ds, current_res, target_resolution)\n",
    "    else:\n",
    "        print(\"→ Step 1: No initial regridding needed\")\n",
    "        regridded = ds\n",
    "    \n",
    "    # Step 2: Final interpolation to exact target resolution\n",
    "    intermediate_res = get_grid_resolution(regridded)\n",
    "    print(f\"→ Step 2: Final interpolation to exact {target_resolution}°\")\n",
    "    print(f\"  Intermediate resolution: {intermediate_res:.6f}°\")\n",
    "    \n",
    "    final_regridded = final_interpolation_to_target(regridded, target_resolution)\n",
    "    \n",
    "    # Validate results\n",
    "    validate_regridding(ds, final_regridded, dataset_name)\n",
    "    \n",
    "    # Close datasets\n",
    "    ds.close()\n",
    "    if regridded is not ds:\n",
    "        regridded.close()\n",
    "    \n",
    "    return final_regridded\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main processing function.\"\"\"\n",
    "    print(\"Starting regridding process...\")\n",
    "    print(f\"Target resolution: {TARGET_RESOLUTION}°\")\n",
    "    \n",
    "    # Process each dataset\n",
    "    regridded_datasets = {}\n",
    "    \n",
    "    for dataset_name, file_path in file_paths.items():\n",
    "        regridded = process_dataset(file_path, dataset_name, TARGET_RESOLUTION)\n",
    "        if regridded is not None:\n",
    "            regridded_datasets[dataset_name] = regridded\n",
    "    \n",
    "    # Save regridded datasets\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"SAVING REGRIDDED DATASETS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    for dataset_name, regridded in regridded_datasets.items():\n",
    "        output_file = output_paths[dataset_name]\n",
    "        print(f\"Saving {dataset_name} to {output_file}...\")\n",
    "        \n",
    "        try:\n",
    "            # Add attributes to document the regridding\n",
    "            regridded.attrs['regridding_info'] = f'Regridded to {TARGET_RESOLUTION}° resolution'\n",
    "            regridded.attrs['regridding_method'] = 'xarray coarsen/interp'\n",
    "            regridded.attrs['processing_date'] = str(np.datetime64('now'))\n",
    "            \n",
    "            # Save with compression\n",
    "            encoding = {}\n",
    "            for var in regridded.data_vars:\n",
    "                encoding[var] = {'zlib': True, 'complevel': 4}\n",
    "            \n",
    "            regridded.to_netcdf(output_file, encoding=encoding)\n",
    "            print(f\"✓ Successfully saved {output_file}\")\n",
    "            \n",
    "            # Print file info\n",
    "            file_size = regridded.nbytes / (1024**2)  # MB\n",
    "            print(f\"  File size: ~{file_size:.1f} MB\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error saving {output_file}: {e}\")\n",
    "        \n",
    "        # Close dataset\n",
    "        regridded.close()\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"REGRIDDING COMPLETE!\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(\"All datasets have been regridded to 0.10° resolution.\")\n",
    "    print(\"You can now use these files for comparative analysis.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example of how to customize for specific variable names\n",
    "    # Uncomment and modify if your datasets have different variable names\n",
    "    \n",
    "    # For MODIS Chlor_a - common variable names: 'chlor_a', 'CHL', 'chl'\n",
    "    # For OSTIA SST - common variable names: 'analysed_sst', 'sst', 'SST'  \n",
    "    # For SMAP SSS - common variable names: 'sss', 'SSS', 'salinity'\n",
    "    \n",
    "    main()\n",
    "\n",
    "    # Optional: Quick validation plot (uncomment if you want visual validation)\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Load one of the regridded files for quick visual check\n",
    "    test_file = output_paths['sst']  # or whichever you want to check\n",
    "    ds_test = xr.open_dataset(test_file)\n",
    "    \n",
    "    # Plot first time step\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    ds_test.isel(time=0).plot()\n",
    "    plt.title(f'Regridded data - First time step')\n",
    "    plt.show()\n",
    "    \n",
    "    ds_test.close()\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf57831f-d496-40d2-8c03-746f98d61be5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'install' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43minstall\u001b[49m\u001b[38;5;241m.\u001b[39mpackages(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevtools\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'install' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d24a8c1-0d47-44e1-af44-66d8a4ad9444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
